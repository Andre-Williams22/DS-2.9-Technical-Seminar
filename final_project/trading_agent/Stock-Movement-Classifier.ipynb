{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: backtesting in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (0.3.1)\n",
      "Requirement already satisfied: yfinance in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (0.1.59)\n",
      "Requirement already satisfied: numpy in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from backtesting) (1.20.2)\n",
      "Requirement already satisfied: bokeh>=1.4.0 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from backtesting) (2.3.1)\n",
      "Requirement already satisfied: pandas!=0.25.0,>=0.25.0 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from backtesting) (1.2.3)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from bokeh>=1.4.0->backtesting) (8.1.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from bokeh>=1.4.0->backtesting) (3.7.4.3)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from bokeh>=1.4.0->backtesting) (5.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from bokeh>=1.4.0->backtesting) (2.8.1)\n",
      "Requirement already satisfied: Jinja2>=2.7 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from bokeh>=1.4.0->backtesting) (2.11.3)\n",
      "Requirement already satisfied: packaging>=16.8 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from bokeh>=1.4.0->backtesting) (20.9)\n",
      "Requirement already satisfied: tornado>=5.1 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from bokeh>=1.4.0->backtesting) (6.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from Jinja2>=2.7->bokeh>=1.4.0->backtesting) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from packaging>=16.8->bokeh>=1.4.0->backtesting) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from pandas!=0.25.0,>=0.25.0->backtesting) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from python-dateutil>=2.1->bokeh>=1.4.0->backtesting) (1.15.0)\n",
      "Requirement already satisfied: lxml>=4.5.1 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from yfinance) (4.6.3)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from yfinance) (2.25.1)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from yfinance) (0.0.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from requests>=2.20->yfinance) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from requests>=2.20->yfinance) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from requests>=2.20->yfinance) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from requests>=2.20->yfinance) (2020.12.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install backtesting yfinance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "# models \n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from yahoofinancials import YahooFinancials as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: alpha_vantage in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (2.3.1)\n",
      "Requirement already satisfied: aiohttp in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from alpha_vantage) (3.7.4.post0)\n",
      "Requirement already satisfied: requests in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from alpha_vantage) (2.25.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from aiohttp->alpha_vantage) (20.3.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from aiohttp->alpha_vantage) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from aiohttp->alpha_vantage) (3.7.4.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from aiohttp->alpha_vantage) (1.6.3)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from aiohttp->alpha_vantage) (4.0.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from aiohttp->alpha_vantage) (5.1.0)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from yarl<2.0,>=1.0->aiohttp->alpha_vantage) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from requests->alpha_vantage) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from requests->alpha_vantage) (1.26.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install alpha_vantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alphavantage Intraday Data up to 2 years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "def alpha_vantage_data(symbol, interval):\n",
    "    apiKey = 'IOLIXAXKGPI3A4QM'\n",
    "\n",
    "    ts = TimeSeries(key = apiKey, output_format = 'csv')\n",
    "\n",
    "    data_frames = []\n",
    "\n",
    "    #download the csv\n",
    "    month_one = ts.get_intraday_extended(symbol = symbol , interval = interval , slice ='year1month1')\n",
    "    one = pd.DataFrame(list(month_one[0]))\n",
    "    data_frames.append(one)\n",
    "\n",
    "    month_two = ts.get_intraday_extended(symbol = symbol, interval = interval, slice ='year1month2')\n",
    "    month_two = pd.DataFrame(list(month_two[0]))\n",
    "    data_frames.append(month_two)\n",
    "    \n",
    "    month_three = ts.get_intraday_extended(symbol = symbol, interval = interval, slice ='year1month3')\n",
    "    month_three = pd.DataFrame(list(month_three[0]))\n",
    "    data_frames.append(month_three)\n",
    "    \n",
    "    month_four = ts.get_intraday_extended(symbol = symbol, interval = interval, slice ='year1month4')\n",
    "    month_four = pd.DataFrame(list(month_four[0]))\n",
    "    data_frames.append(month_four)\n",
    "    \n",
    "    month_five = ts.get_intraday_extended(symbol = symbol, interval = interval, slice ='year1month5')\n",
    "    month_five = pd.DataFrame(list(month_five[0]))\n",
    "    data_frames.append(month_five)\n",
    "    \n",
    "#     month_six = ts.get_intraday_extended(symbol = symbol, interval = interval, slice ='year1month6')\n",
    "#     month_six = pd.DataFrame(list(month_six[0]))\n",
    "#     data_frames.append(month_six)\n",
    "    \n",
    "#     month_seven = ts.get_intraday_extended(symbol = symbol, interval = interval, slice ='year1month7')\n",
    "#     month_seven = pd.DataFrame(list(month_seven[0]))\n",
    "#     data_frames.append(month_seven)\n",
    "    \n",
    "    \n",
    "#     month_eight = ts.get_intraday_extended(symbol = symbol, interval = interval, slice ='year1month8')\n",
    "#     month_eight = pd.DataFrame(list(month_eight[0]))\n",
    "#     data_frames.append(month_eight)\n",
    "\n",
    "#     month_nine = ts.get_intraday_extended(symbol = symbol, interval = interval, slice ='year1month9')\n",
    "#     month_nine = pd.DataFrame(list(month_nine[0]))\n",
    "#     data_frames.append(month_nine)\n",
    "    \n",
    "#     month_ten = ts.get_intraday_extended(symbol = symbol, interval = interval, slice ='year1month10')\n",
    "#     month_ten = pd.DataFrame(list(month_ten[0]))\n",
    "#     data_frames.append(month_ten)\n",
    "    \n",
    "#     month_eleven = ts.get_intraday_extended(symbol = symbol, interval = interval, slice ='year1month11')\n",
    "#     month_eleven = pd.DataFrame(list(month_eleven[0]))\n",
    "#     data_frames.append(month_eleven) \n",
    "    \n",
    "#     month_twelve = ts.get_intraday_extended(symbol = symbol, interval = interval, slice ='year1month12')\n",
    "#     month_twelve = pd.DataFrame(list(month_twelve[0]))\n",
    "#     data_frames.append(month_twelve)\n",
    "    \n",
    "    df = pd.concat(data_frames)\n",
    "    \n",
    "#     #df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['DATE'], how='outer'), data_frames).fillna('void')\n",
    "\n",
    "    #csv --> dataframe\n",
    "#     df = pd.DataFrame(list(df_merged[0]))\n",
    "\n",
    "    # print(df)\n",
    "\n",
    "    #setup of column and index\n",
    "    header_row=0\n",
    "    df.columns = df.iloc[header_row]\n",
    "    df = df.drop(header_row)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    #show output\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-12-14 05:15:00</th>\n",
       "      <td>27.81</td>\n",
       "      <td>27.93</td>\n",
       "      <td>27.81</td>\n",
       "      <td>27.92</td>\n",
       "      <td>3402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-14 05:00:00</th>\n",
       "      <td>27.84</td>\n",
       "      <td>27.96</td>\n",
       "      <td>27.8</td>\n",
       "      <td>27.81</td>\n",
       "      <td>10669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-14 04:45:00</th>\n",
       "      <td>27.77</td>\n",
       "      <td>27.85</td>\n",
       "      <td>27.75</td>\n",
       "      <td>27.83</td>\n",
       "      <td>8862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-14 04:30:00</th>\n",
       "      <td>27.73</td>\n",
       "      <td>27.84</td>\n",
       "      <td>27.71</td>\n",
       "      <td>27.76</td>\n",
       "      <td>12888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-14 04:15:00</th>\n",
       "      <td>27.48</td>\n",
       "      <td>27.8</td>\n",
       "      <td>27.31</td>\n",
       "      <td>27.71</td>\n",
       "      <td>15174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0                     open   high    low  close volume\n",
       "time                                                  \n",
       "2020-12-14 05:15:00  27.81  27.93  27.81  27.92   3402\n",
       "2020-12-14 05:00:00  27.84  27.96   27.8  27.81  10669\n",
       "2020-12-14 04:45:00  27.77  27.85  27.75  27.83   8862\n",
       "2020-12-14 04:30:00  27.73  27.84  27.71  27.76  12888\n",
       "2020-12-14 04:15:00  27.48   27.8  27.31  27.71  15174"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = alpha_vantage_data('PLTR', '15min')\n",
    "\n",
    "df.tail()\n",
    "## combine dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 6569 entries, 2021-05-12 20:00:00 to 2020-12-14 04:15:00\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   open    6569 non-null   float64\n",
      " 1   high    6569 non-null   float64\n",
      " 2   low     6569 non-null   float64\n",
      " 3   close   6569 non-null   float64\n",
      " 4   volume  6569 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 307.9 KB\n"
     ]
    }
   ],
   "source": [
    "df = df.apply(pd.to_numeric)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6569"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yahoo Finance Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking the stock to predict and get historical data\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "\n",
    "def get_stock_data(tickers, start_date='', end_date='', freq='daily'):\n",
    "    '''This function takes the list of stock ticker and get historical OHLC data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tickers : list/iterable\n",
    "        Iterable object containing ticker symbols as strings\n",
    "    start_date : str, optional\n",
    "        Takes start date of data, format = 'yyyy-mm-dd'\n",
    "    end_date : str, optional\n",
    "        Takes end date of data, format = 'yyyy-mm-dd'\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame containing pricing data and list containing tickers whose data was not found\n",
    "    \n",
    "    '''\n",
    "\n",
    "#     ticker_not_found=[]\n",
    "#     for ticker in tickers:\n",
    "#         yf_engine = yf(ticker)\n",
    "#         price = yf_engine.get_historical_price_data(start_date,end_date,freq)\n",
    "#         #store the data in DataFrame\n",
    "#         try:\n",
    "#             ticker_data = pd.DataFrame(price[ticker]['prices'])\n",
    "#             ticker_data = ticker_data.drop('date', axis=1) # We will use formatted_date columns instead\n",
    "#         except:\n",
    "#             ticker_not_found.append(ticker)\n",
    "#             continue\n",
    "            \n",
    "#     return ticker_data, ticker_not_found\n",
    "\n",
    "\n",
    "\n",
    "    # import mplfinance as mpf\n",
    "    # from datetime import datetime\n",
    "    \n",
    "    start_date = datetime.datetime.now() - datetime.timedelta(59)\n",
    "    current_date = datetime.datetime.today()\n",
    "    # sd = datetime(2021, 2, 5)\n",
    "    # ed = datetime(2021, 4, 2)\n",
    "    for ticker in tickers:\n",
    "        data = yf.download(tickers=ticker, start=start_date, end=current_date, interval=\"15m\")\n",
    "    # mpf.plot(df,type='candle',mav=(3,6,9),volume=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/andrewilliams/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/andrewilliams/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/multitasking/__init__.py\", line 102, in _run_via_pool\n",
      "    return callee(*args, **kwargs)\n",
      "  File \"/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/yfinance/multi.py\", line 167, in _download_one_threaded\n",
      "    data = _download_one(ticker, start, end, auto_adjust, back_adjust,\n",
      "  File \"/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/yfinance/multi.py\", line 179, in _download_one\n",
      "    return Ticker(ticker).history(period=period, interval=interval,\n",
      "  File \"/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/yfinance/base.py\", line 221, in history\n",
      "    quotes.dropna(inplace=True)\n",
      "  File \"/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/pandas/core/frame.py\", line 5170, in dropna\n",
      "    ...                                   11300, 11300],\n",
      "  File \"/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/pandas/core/ops/common.py\", line 65, in new_method\n",
      "    \n",
      "  File \"/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/pandas/core/arraylike.py\", line 29, in __eq__\n",
      "  File \"/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/pandas/core/series.py\", line 4978, in _cmp_method\n",
      "  File \"/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\", line 250, in comparison_op\n",
      "    op_name = f\"__{op.__name__}__\"\n",
      "  File \"/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\", line 139, in _na_arithmetic_op\n",
      "    -------\n",
      "  File \"/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/pandas/core/computation/expressions.py\", line 15, in <module>\n",
      "    from pandas._libs.lib import values_from_object\n",
      "ImportError: cannot import name 'values_from_object' from 'pandas._libs.lib' (/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/pandas/_libs/lib.cpython-38-darwin.so)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start_date = datetime.datetime.now() - datetime.timedelta(59)\n",
    "current_date = datetime.datetime.today()\n",
    "#We will use Google stock ticker to predict its prices\n",
    "ticker = ['CF']\n",
    "# start_date = '2020-09-01'\n",
    "# end_date = '2021-4-02'\n",
    "\n",
    "# df, ticker_not_found = get_stock_data(ticker, start_date, end_date)\n",
    "\n",
    "df = get_stock_data(ticker)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab Alpaca Intraday Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install alpaca-trade-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alpaca_trade_api as tradeapi\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Grab alpaca intraday data \n",
    "# # import alpaca_trade_api as tradeapi\n",
    "\n",
    "# endpoint = \"https://data.alpaca.markets/v1\"\n",
    "# headers = {\n",
    "#     \"APCA-API-KEY-ID\": \"PKHOA1VIMFOC6P6YFNS8\",\n",
    "#     \"APCA-API-SECRET-KEY\": \"BIZJBuginzyMC3JpPdBvRqTWVkb1Xol6FjSuaFZN\"\n",
    "# }\n",
    "\n",
    "# #json.loads(open(\"account.json\",'r').read())\n",
    "# api = tradeapi.REST(headers[\"APCA-API-KEY-ID\"], headers[\"APCA-API-SECRET-KEY\"], base_url='https://paper-api.alpaca.markets')\n",
    "\n",
    "# def hist_data(symbol, dataframe, timeframe=\"15Min\", limit=1000, start=\"\", end=\"\", after=\"\", until=\"\"):\n",
    "#     '''Returns the historical bar data for a group of stocks '''\n",
    "#     df_data = {}\n",
    "#     # Get Requests for Bar Data\n",
    "#     bar_url = endpoint + \"/bars/{}\".format(timeframe)\n",
    "\n",
    "\n",
    "#     params = {\n",
    "#         \"symbols\": symbol,\n",
    "#         \"start\": start,\n",
    "#         \"end\": end,\n",
    "#         \"limit\": limit,\n",
    "#         \"timeframe\": timeframe\n",
    "#     }\n",
    "\n",
    "#     r = requests.get(bar_url, headers=headers, params=params)\n",
    "\n",
    "#     json_dump = r.json()\n",
    "#     # loop through stock data\n",
    "#     for symbol in json_dump:\n",
    "#         # convert json into pandas dataframe\n",
    "#         temp = pd.DataFrame(json_dump[symbol])\n",
    "#         temp.rename({\"t\":\"datadate\", \"o\": \"open\", 'h':'high', 'l':'low', \"c\":\"close\", 'v':'volume'}, axis=1, inplace=True)\n",
    "\n",
    "#         temp['tic'] = symbol\n",
    "\n",
    "#         dataframe = pd.concat([dataframe, temp])\n",
    "\n",
    "#     return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #limit = 2 \n",
    "# def getData(tickers, start=\"\", end=\"\", limit=20):\n",
    "#     dataframes = dict()\n",
    "#     for symbol in tickers:\n",
    "#         dataframes[symbol] = pd.DataFrame()#columns = [symbol])\n",
    "\n",
    "#     # time is in seconds\n",
    "\n",
    "#     # starttime = time.time()\n",
    "#     # # will go for 8 hours\n",
    "#     # # timeout = starttime + 60 * 5 # 8 hrs 60 * 8\n",
    "#     # timeout = starttime + 1\n",
    "\n",
    "#     # while time.time() <= timeout:\n",
    "#     # print(\"****************************************************\")\n",
    "#     for company in tickers:\n",
    "#         # print(\"printing data for {} at {}\".format(company, time.time()))\n",
    "#         dataframes[company] = hist_data(company, dataframes[company], '15Min', start=start, end=end)\n",
    "\n",
    "#     final_prices = dataframes[tickers[0]]\n",
    "# #     print(final_prices)\n",
    "    \n",
    "#     for key, value in dataframes.items():\n",
    "# #         print('value',value)\n",
    "#         final_prices = pd.concat([final_prices, value])\n",
    "\n",
    "\n",
    "#     final_prices = final_prices.sort_values(by=['datadate'])\n",
    "\n",
    "# #     print('final prices: \\n',final_prices)\n",
    "\n",
    "# #     for index, row in final_prices.iterrows():\n",
    "# #         year = str(datetime.fromtimestamp(int(row['datadate'])).year)\n",
    "# #         month = str(datetime.fromtimestamp(int(row['datadate'])).month)\n",
    "# #         #minute = str(datetime.fromtimestamp(int(row['datadate'])).minute)\n",
    "        \n",
    "# #         if len(month) == 1:\n",
    "# #             month = \"0\" + month\n",
    "# #         day = str(datetime.fromtimestamp(int(row['datadate'])).day)\n",
    "# #         if len(day) == 1:\n",
    "# #             day = \"0\" + day\n",
    "# #         # print(year+month+day)\n",
    "# #         # row['datadate'] = year+month+day\n",
    "# #         final_prices.at[index, 'datadate'] = year+month+day#+minute\n",
    "\n",
    "# #     final_prices = final_prices.reset_index()\n",
    "# #     final_prices = final_prices.drop([\"index\"], axis=1)\n",
    "# #     final_prices = final_prices.drop_duplicates()\n",
    "    \n",
    "#     print('final prices: \\n',final_prices)\n",
    "    \n",
    "#     return final_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = getData(['PLTR','AAPL'])\n",
    "\n",
    "# df.head()\n",
    "\n",
    "# df['adjclose'] = df['close']\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['date'] = pd.to_datetime(df['formatted_date'])\n",
    "# df = df.set_index(pd.DatetimeIndex(df['date'])) \n",
    "# df.drop(df.columns[-1], axis=1, inplace=True)\n",
    "# df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.columns = ['open', 'high', 'low', 'close', 'volume', 'adjclose']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isna()==True].sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['volume'].plot(label='Volume', color='r');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['adjclose'].plot(label='Closing Price', color='g');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create certain features from the pricing data which would include:\n",
    "1. SMA_20 : Simple Moving Average of 20 day window\n",
    "2. Std_dev : Standard Deviation for 20 day window\n",
    "3. Band_1 : Bollinger band created using SMA_20 + Std_dev\n",
    "4. Band_2 : Bollinger band created using SMA_20 - Std_dev\n",
    "5. ON_returns : whether there was up or down move from prior day closing price to current day opening price\n",
    "6. dist_from_mean : How much distant stock prices are from the mean\n",
    "7. vix_data: CBOE Volatility index price from the prior day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BBANDS(data, n_lookback, n_std):\n",
    "    \"\"\"Bollinger bands indicator\"\"\"\n",
    "    hlc3 = (data.high + data.low + data.close) / 3\n",
    "    mean, std = hlc3.rolling(n_lookback).mean(), hlc3.rolling(n_lookback).std()\n",
    "    upper = mean + n_std*std\n",
    "    lower = mean - n_std*std\n",
    "    return upper, lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backtesting.test import EURUSD, SMA\n",
    "\n",
    "def get_clean_data (df, start_date=None, end_date=None):\n",
    "    '''This function takes the historical OHLC data and return features as we defined above\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Dataframe containing pricing data\n",
    "    start_date : str, optional\n",
    "        Takes start date of data for vix, format = 'yyyy-mm-dd'\n",
    "    end_date : str, optional\n",
    "        Takes ebd date of data for vix, format = 'yyyy-mm-dd'\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame containing scaled features except categorical features\n",
    "    '''\n",
    "    \n",
    "    features = df.copy()\n",
    "    # Some datetime features for good measure\n",
    "    features['X_day'] = features.index.dayofweek\n",
    "    features['X_hour'] = features.index.hour\n",
    "#     features = features.drop(['formatted_date'], axis=1)\n",
    "    #creating features as stated above\n",
    "    features['volume'] = features['volume'].shift(1)\n",
    "    close = features.close.values\n",
    "    sma10 = SMA(features.close, 10)\n",
    "    sma20 = SMA(features.close, 20)\n",
    "    sma50 = SMA(features.close, 50)\n",
    "    sma100 = SMA(features.close, 100)\n",
    "    upper, lower = BBANDS(features, 20, 2)\n",
    "#     upper, lower = BBANDS(features, 20, 2)\n",
    "\n",
    "    # Design matrix / independent features:\n",
    "\n",
    "    # Price-derived features\n",
    "    features['X_SMA10'] = (close - sma10) / close\n",
    "    features['X_SMA20'] = (close - sma20) / close\n",
    "    features['X_SMA50'] = (close - sma50) / close\n",
    "    features['X_SMA100'] = (close - sma100) / close\n",
    "\n",
    "    features['X_DELTA_SMA10'] = (sma10 - sma20) / close\n",
    "    features['X_DELTA_SMA20'] = (sma20 - sma50) / close\n",
    "    features['X_DELTA_SMA50'] = (sma50 - sma100) / close\n",
    "\n",
    "    # Indicator features\n",
    "    features['X_MOM'] = features.close.pct_change(periods=2)\n",
    "    features['X_BB_upper'] = (upper - close) / close\n",
    "    features['X_BB_lower'] = (lower - close) / close\n",
    "    features['X_BB_width'] = (upper - lower) / close\n",
    "    \n",
    "    \n",
    "    features['SMA'] = features['adjclose'].rolling(window=20).mean().shift(1)\n",
    "    features['Std_20'] = features['adjclose'].rolling(window=20).std().shift(1)\n",
    "    features['Band_1'] = features['SMA'] - features['Std_20']\n",
    "    features['Band_2'] = features['SMA'] + features['Std_20']\n",
    "    features['ON_returns'] = features['close'] - features['open'].shift(-1)\n",
    "    features['ON_returns'] = features['ON_returns'].shift(1)\n",
    "    features['ON_returns_signal'] = np.where(features['ON_returns']<0, 'up', 'down')\n",
    "    features['dist_from_mean'] = features['adjclose'].shift(1) - features['X_SMA10']\n",
    "    \n",
    "#     print('features\\n', features.head())\n",
    "    \n",
    "#     #Obtaining Vix Data and combining with existing features of stock\n",
    "#     ticker = ['^VIX']\n",
    "#     start_date = start_date\n",
    "#     end_date = end_date\n",
    "#     vix_data, ticker_not_found = get_stock_data(ticker, start_date, end_date)\n",
    "#     vix_data = pd.DataFrame(vix_data['adjclose'].shift(1))\n",
    "#     vix_data = vix_data.rename(columns = {'adjclose':'vix_data'})\n",
    "#     comb_features = pd.concat([features,vix_data], axis=1)\n",
    "#     comb_features = comb_features.dropna() #dropping NaN values\n",
    "    comb_features = pd.get_dummies(features, columns=['ON_returns_signal']) #for categorical variables\n",
    "    comb_features = comb_features.drop('ON_returns', axis=1) #dropping original categorical column\n",
    "    comb_features = comb_features.drop('close', axis=1) #not really needed this value since we have adj close now\n",
    "    ###Create return column to predict\n",
    "    comb_features['stock_move'] = np.where(comb_features['adjclose']-\n",
    "                                           comb_features['adjclose'].shift(-1)<0, \"Buy\", \"Sell\")\n",
    "    features_clean = comb_features.dropna() #Dropping Nan values\n",
    "    features_clean = features_clean[:-1] #Drop last row which do not have any stock signal\n",
    "    features_clean.tail()\n",
    "    return features_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = get_clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.drop(['ON_returns_signal_down','ON_returns_signal_up'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_table = features.drop(['high', 'low'], axis=1).corr()\n",
    "fig, ax = plt.subplots(figsize=(18,6))\n",
    "sns.heatmap(corr_table, annot = True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the Data: Train-test-split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_X(data):\n",
    "    \"\"\"Return model design matrix X\"\"\"\n",
    "    return data.filter(like='X').values\n",
    "\n",
    "\n",
    "def get_y(data):\n",
    "    \"\"\"Return dependent variable y\"\"\"\n",
    "    y = data.adjclose.pct_change(48).shift(-48)  # Returns after roughly two days\n",
    "    y[y.between(-.004, .004)] = 0             # Devalue returns smaller than 0.4%\n",
    "    y[y > 0] = 1\n",
    "    y[y < 0] = -1\n",
    "    return y\n",
    "\n",
    "\n",
    "def get_clean_Xy(df):\n",
    "    \"\"\"Return (X, y) cleaned of NaN values\"\"\"\n",
    "    X = get_X(df)\n",
    "    y = get_y(df).values\n",
    "    isnan = np.isnan(y)\n",
    "    X = X[~isnan]\n",
    "    y = y[~isnan]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(40)\n",
    "\n",
    "X, y = get_clean_Xy(features)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(944, 13)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(472, 13)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## Load in model \n",
    "# import pickle\n",
    "# with open('Optimized-Extra-Classifier-89', 'rb') as file:\n",
    "#     oec = pickle.load(file)\n",
    "    \n",
    "\n",
    "# # print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "# y_predict = oec.predict(X)\n",
    "\n",
    "# y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "labelencoder= LabelEncoder() #initializing an object of class LabelEncoder\n",
    "features['stock_move'] = labelencoder.fit_transform(features['stock_move']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features.drop(['stock_move'], axis=1)\n",
    "y = features['stock_move']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datetime\n",
       "2021-03-11 14:45:00-05:00    1\n",
       "2021-03-11 15:00:00-05:00    0\n",
       "2021-03-11 15:15:00-05:00    1\n",
       "2021-03-11 15:30:00-05:00    0\n",
       "2021-03-11 15:45:00-05:00    1\n",
       "                            ..\n",
       "2021-05-05 14:30:00-04:00    0\n",
       "2021-05-05 14:45:00-04:00    0\n",
       "2021-05-05 15:00:00-04:00    0\n",
       "2021-05-05 15:15:00-04:00    0\n",
       "2021-05-05 15:30:00-04:00    0\n",
       "Name: stock_move, Length: 992, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>volume</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>X_day</th>\n",
       "      <th>X_hour</th>\n",
       "      <th>X_SMA10</th>\n",
       "      <th>X_SMA20</th>\n",
       "      <th>X_SMA50</th>\n",
       "      <th>...</th>\n",
       "      <th>X_MOM</th>\n",
       "      <th>X_BB_upper</th>\n",
       "      <th>X_BB_lower</th>\n",
       "      <th>X_BB_width</th>\n",
       "      <th>SMA</th>\n",
       "      <th>Std_20</th>\n",
       "      <th>Band_1</th>\n",
       "      <th>Band_2</th>\n",
       "      <th>dist_from_mean</th>\n",
       "      <th>stock_move</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-03-11 14:45:00-05:00</th>\n",
       "      <td>50.500000</td>\n",
       "      <td>50.575100</td>\n",
       "      <td>50.444500</td>\n",
       "      <td>50.500000</td>\n",
       "      <td>73539</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.019615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002383</td>\n",
       "      <td>0.004796</td>\n",
       "      <td>-0.005929</td>\n",
       "      <td>0.010725</td>\n",
       "      <td>79505.40</td>\n",
       "      <td>55595.685179</td>\n",
       "      <td>23909.714821</td>\n",
       "      <td>135101.085179</td>\n",
       "      <td>60597.997840</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-11 15:00:00-05:00</th>\n",
       "      <td>50.470001</td>\n",
       "      <td>50.509899</td>\n",
       "      <td>50.299999</td>\n",
       "      <td>50.470001</td>\n",
       "      <td>67765</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000618</td>\n",
       "      <td>0.016970</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002475</td>\n",
       "      <td>0.005782</td>\n",
       "      <td>-0.003765</td>\n",
       "      <td>0.009547</td>\n",
       "      <td>71155.15</td>\n",
       "      <td>40674.806465</td>\n",
       "      <td>30480.343535</td>\n",
       "      <td>111829.956465</td>\n",
       "      <td>73538.999970</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-11 15:15:00-05:00</th>\n",
       "      <td>50.375000</td>\n",
       "      <td>50.520000</td>\n",
       "      <td>50.270000</td>\n",
       "      <td>50.375000</td>\n",
       "      <td>138634</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.017122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000793</td>\n",
       "      <td>0.004695</td>\n",
       "      <td>-0.004818</td>\n",
       "      <td>0.009513</td>\n",
       "      <td>63832.30</td>\n",
       "      <td>22832.622834</td>\n",
       "      <td>40999.677166</td>\n",
       "      <td>86664.922834</td>\n",
       "      <td>67764.999197</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-11 15:30:00-05:00</th>\n",
       "      <td>50.410000</td>\n",
       "      <td>50.494999</td>\n",
       "      <td>50.340000</td>\n",
       "      <td>50.430000</td>\n",
       "      <td>77822</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.000427</td>\n",
       "      <td>-0.000429</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.005595</td>\n",
       "      <td>-0.003882</td>\n",
       "      <td>0.009477</td>\n",
       "      <td>65542.25</td>\n",
       "      <td>26943.880410</td>\n",
       "      <td>38598.369590</td>\n",
       "      <td>92486.130410</td>\n",
       "      <td>138634.000427</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-11 15:45:00-05:00</th>\n",
       "      <td>50.375000</td>\n",
       "      <td>50.590000</td>\n",
       "      <td>50.299999</td>\n",
       "      <td>50.380001</td>\n",
       "      <td>251564</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.017569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002082</td>\n",
       "      <td>0.002324</td>\n",
       "      <td>-0.006894</td>\n",
       "      <td>0.009218</td>\n",
       "      <td>64592.70</td>\n",
       "      <td>26105.434694</td>\n",
       "      <td>38487.265306</td>\n",
       "      <td>90698.134694</td>\n",
       "      <td>77821.997744</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                open       high        low     volume  \\\n",
       "Datetime                                                                \n",
       "2021-03-11 14:45:00-05:00  50.500000  50.575100  50.444500  50.500000   \n",
       "2021-03-11 15:00:00-05:00  50.470001  50.509899  50.299999  50.470001   \n",
       "2021-03-11 15:15:00-05:00  50.375000  50.520000  50.270000  50.375000   \n",
       "2021-03-11 15:30:00-05:00  50.410000  50.494999  50.340000  50.430000   \n",
       "2021-03-11 15:45:00-05:00  50.375000  50.590000  50.299999  50.380001   \n",
       "\n",
       "                           adjclose  X_day  X_hour   X_SMA10   X_SMA20  \\\n",
       "Datetime                                                                 \n",
       "2021-03-11 14:45:00-05:00     73539      3      14  0.002160  0.000897   \n",
       "2021-03-11 15:00:00-05:00     67765      3      15  0.000030 -0.000618   \n",
       "2021-03-11 15:15:00-05:00    138634      3      15  0.000803  0.000364   \n",
       "2021-03-11 15:30:00-05:00     77822      3      15 -0.000427 -0.000429   \n",
       "2021-03-11 15:45:00-05:00    251564      3      15  0.002256  0.002666   \n",
       "\n",
       "                            X_SMA50  ...     X_MOM  X_BB_upper  X_BB_lower  \\\n",
       "Datetime                             ...                                     \n",
       "2021-03-11 14:45:00-05:00  0.019615  ...  0.002383    0.004796   -0.005929   \n",
       "2021-03-11 15:00:00-05:00  0.016970  ... -0.002475    0.005782   -0.003765   \n",
       "2021-03-11 15:15:00-05:00  0.017122  ... -0.000793    0.004695   -0.004818   \n",
       "2021-03-11 15:30:00-05:00  0.015500  ...  0.000099    0.005595   -0.003882   \n",
       "2021-03-11 15:45:00-05:00  0.017569  ...  0.002082    0.002324   -0.006894   \n",
       "\n",
       "                           X_BB_width       SMA        Std_20        Band_1  \\\n",
       "Datetime                                                                      \n",
       "2021-03-11 14:45:00-05:00    0.010725  79505.40  55595.685179  23909.714821   \n",
       "2021-03-11 15:00:00-05:00    0.009547  71155.15  40674.806465  30480.343535   \n",
       "2021-03-11 15:15:00-05:00    0.009513  63832.30  22832.622834  40999.677166   \n",
       "2021-03-11 15:30:00-05:00    0.009477  65542.25  26943.880410  38598.369590   \n",
       "2021-03-11 15:45:00-05:00    0.009218  64592.70  26105.434694  38487.265306   \n",
       "\n",
       "                                  Band_2  dist_from_mean  stock_move  \n",
       "Datetime                                                              \n",
       "2021-03-11 14:45:00-05:00  135101.085179    60597.997840           1  \n",
       "2021-03-11 15:00:00-05:00  111829.956465    73538.999970           0  \n",
       "2021-03-11 15:15:00-05:00   86664.922834    67764.999197           1  \n",
       "2021-03-11 15:30:00-05:00   92486.130410   138634.000427           0  \n",
       "2021-03-11 15:45:00-05:00   90698.134694    77821.997744           1  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since predictions are somewhat evenly distributed so there is lesser chance of bias in our predictor model. Having \"Buy\" signals more than sell signal kind of makes sense since Jan, stock price has risen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #Scale the features\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put models in a dictionary\n",
    "models = {\"KNN\": KNeighborsClassifier(),\n",
    "          \"Logistic Regression\": LogisticRegression(), \n",
    "          \"Random Forest\": RandomForestClassifier(),\n",
    "          \"SGD Classifier\": SGDClassifier(), \n",
    "         \"SVM\": svm.SVC(kernel = 'rbf')}\n",
    "\n",
    "# Create function to fit and score models\n",
    "def fit_and_score(models, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Fits and evaluates given machine learning models.\n",
    "    models : a dict of different Scikit-Learn machine learning models\n",
    "    X_train : training data\n",
    "    X_test : testing data\n",
    "    y_train : labels assosciated with training data\n",
    "    y_test : labels assosciated with test data\n",
    "    \"\"\"\n",
    "    # Random seed for reproducible results\n",
    "    np.random.seed(42)\n",
    "    # Make a list to keep model scores\n",
    "    model_scores = {}\n",
    "    # Loop through models\n",
    "    for name, model in models.items():\n",
    "        # Fit the model to the data\n",
    "        model.fit(X_train, y_train)\n",
    "        # Evaluate the model and append its score to model_scores\n",
    "        model_scores[name] = model.score(X_test, y_test)\n",
    "    \n",
    "    print('classification Accuracy')\n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'KNN': 0.6330645161290323,\n",
       " 'Logistic Regression': 0.6149193548387096,\n",
       " 'Random Forest': 0.6008064516129032,\n",
       " 'SGD Classifier': 0.5564516129032258,\n",
       " 'SVM': 0.625}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores = fit_and_score(models=models,\n",
    "                             X_train=X_train,\n",
    "                             X_test=X_test,\n",
    "                             y_train=y_train,\n",
    "                             y_test=y_test)\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_compare = pd.DataFrame(model_scores, index=['accuracy'])\n",
    "model_compare.T.plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different LogisticRegression hyperparameters\n",
    "log_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n",
    "                \"solver\": [\"liblinear\"], \n",
    "               \"max_iter\": np.arange(10, 200, 10)}\n",
    "\n",
    "# Different RandomForestClassifier hyperparameters\n",
    "rf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n",
    "           \"max_depth\": [None, 3, 5, 10],\n",
    "           \"min_samples_split\": np.arange(2, 20, 2),\n",
    "           \"min_samples_leaf\": np.arange(1, 20, 2)}\n",
    "\n",
    "# Different KNN hyperparameters\n",
    "knn_grid = { \"n_neighbors\": np.arange(1, 29, 1), \n",
    "           \"leaf_size\": np.arange(30, 200, 10),\n",
    "           \"weights\": [\"uniform\", \"distance\"],\n",
    "           \"algorithm\": ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup random hyperparameter search for LogisticRegression\n",
    "rs_log_reg = RandomizedSearchCV(LogisticRegression(),\n",
    "                                param_distributions=log_reg_grid,\n",
    "                                cv=5,\n",
    "                                n_iter=20,\n",
    "                                verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_log_reg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    }
   ],
   "source": [
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup random hyperparameter search for RandomForestClassifier\n",
    "rfc = RandomizedSearchCV(RandomForestClassifier(),\n",
    "                           param_distributions=rf_grid,\n",
    "                           cv=5,\n",
    "                           n_iter=50,\n",
    "                           verbose=True)\n",
    "\n",
    "# accuracy ={}\n",
    "# for i in range(1,300):\n",
    "#     classifier = RandomForestClassifier(n_estimators=i, random_state=15)\n",
    "#     classifier.fit(X_train, y_train)\n",
    "#     y_pred_ev = classifier.predict(X_test)\n",
    "#     accuracy[i] = accuracy_score(y_test, y_pred_ev)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rfc.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    }
   ],
   "source": [
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup random hyperparameter search for KNN\n",
    "rs_knn = RandomizedSearchCV(KNeighborsClassifier(),\n",
    "                                param_distributions=knn_grid,\n",
    "                                cv=5,\n",
    "                                n_iter=100,\n",
    "                                verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_knn.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'liblinear', 'max_iter': 70, 'C': 11.288378916846883}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the best parameters\n",
    "rs_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 860,\n",
       " 'min_samples_split': 16,\n",
       " 'min_samples_leaf': 5,\n",
       " 'max_depth': 5}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the best parameters\n",
    "rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weights': 'uniform',\n",
       " 'n_neighbors': 17,\n",
       " 'leaf_size': 140,\n",
       " 'algorithm': 'brute'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_knn.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score New Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7157258064516129"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomized search random forest model\n",
    "rfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6209677419354839"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6713709677419355"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = rfc.predict(X_test)\n",
    "plt.hist(y_pred, bins=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test.hist(bins=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the model is:  0.7157258064516129\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"Accuracy score of the model is: \", accuracy_score(y_test, y_pred))\n",
    "plot_confusion_matrix(rfc, X_test, y_test);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(11 , 6))\n",
    "# plt.plot(list(accuracy.keys()), list(accuracy.values()))\n",
    "# plt.title('Accuracy based on different tree sizes in the forest')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy Score')\n",
    "# plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save rfc \n",
    "## Serialize Optimized Random Forest \n",
    "import pickle \n",
    "pickle.dump(rfc, open('rf-72', 'wb'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Learner Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (3.2.1)\r\n",
      "Requirement already satisfied: wheel in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from lightgbm) (0.36.2)\r\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from lightgbm) (0.24.1)\r\n",
      "Requirement already satisfied: numpy in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from lightgbm) (1.20.2)\r\n",
      "Requirement already satisfied: scipy in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from lightgbm) (1.6.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install lightgbm #xgboost #optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlens.ensemble import SuperLearner\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, Perceptron, PassiveAggressiveClassifier, LogisticRegression, SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, metric, trials=30):\n",
    "        self.metric = metric\n",
    "        self.trials = trials\n",
    "        self.sampler = TPESampler(seed=666)\n",
    "        \n",
    "    def objective(self, trial):\n",
    "        model = create_model(trial)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        if self.metric == 'acc':\n",
    "            return accuracy_score(y_test, preds)\n",
    "        else:\n",
    "            return f1_score(y_test, preds)\n",
    "            \n",
    "    def optimize(self):\n",
    "        study = optuna.create_study(direction=\"maximize\", sampler=self.sampler)\n",
    "        study.optimize(self.objective, n_trials=self.trials)\n",
    "        return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:15:52,377]\u001b[0m A new study created in memory with name: no-name-25a14c74-b429-4842-860e-7e8e70626e32\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest accuracy:  0.7760910815939279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:15:52,804]\u001b[0m Trial 0 finished with value: 0.6030993042378242 and parameters: {'max_depth': 5, 'n_estimators': 127, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.6030993042378242.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:53,277]\u001b[0m Trial 1 finished with value: 0.6030993042378242 and parameters: {'max_depth': 5, 'n_estimators': 143, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.6030993042378242.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:53,312]\u001b[0m Trial 2 finished with value: 0.5721062618595826 and parameters: {'max_depth': 4, 'n_estimators': 9, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.6030993042378242.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:53,420]\u001b[0m Trial 3 finished with value: 0.5692599620493358 and parameters: {'max_depth': 4, 'n_estimators': 31, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.6030993042378242.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:53,658]\u001b[0m Trial 4 finished with value: 0.5493358633776091 and parameters: {'max_depth': 2, 'n_estimators': 106, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.6030993042378242.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:53,667]\u001b[0m Trial 5 finished with value: 0.5702087286527514 and parameters: {'max_depth': 5, 'n_estimators': 2, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.6030993042378242.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:53,760]\u001b[0m Trial 6 finished with value: 0.5455407969639469 and parameters: {'max_depth': 2, 'n_estimators': 38, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.6030993042378242.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:53,948]\u001b[0m Trial 7 finished with value: 0.5901328273244781 and parameters: {'max_depth': 5, 'n_estimators': 52, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.6030993042378242.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:54,529]\u001b[0m Trial 8 finished with value: 0.6220746363061354 and parameters: {'max_depth': 6, 'n_estimators': 147, 'min_samples_leaf': 6}. Best is trial 8 with value: 0.6220746363061354.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:54,782]\u001b[0m Trial 9 finished with value: 0.5585072738772928 and parameters: {'max_depth': 3, 'n_estimators': 88, 'min_samples_leaf': 4}. Best is trial 8 with value: 0.6220746363061354.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:55,323]\u001b[0m Trial 10 finished with value: 0.6173308032890575 and parameters: {'max_depth': 6, 'n_estimators': 150, 'min_samples_leaf': 5}. Best is trial 8 with value: 0.6220746363061354.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:55,844]\u001b[0m Trial 11 finished with value: 0.6217583807716635 and parameters: {'max_depth': 6, 'n_estimators': 148, 'min_samples_leaf': 6}. Best is trial 8 with value: 0.6220746363061354.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:56,309]\u001b[0m Trial 12 finished with value: 0.6195445920303605 and parameters: {'max_depth': 6, 'n_estimators': 122, 'min_samples_leaf': 6}. Best is trial 8 with value: 0.6220746363061354.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:56,860]\u001b[0m Trial 13 finished with value: 0.6201771030993042 and parameters: {'max_depth': 6, 'n_estimators': 144, 'min_samples_leaf': 10}. Best is trial 8 with value: 0.6220746363061354.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:57,242]\u001b[0m Trial 14 finished with value: 0.620809614168248 and parameters: {'max_depth': 6, 'n_estimators': 90, 'min_samples_leaf': 9}. Best is trial 8 with value: 0.6220746363061354.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:57,769]\u001b[0m Trial 15 finished with value: 0.6198608475648324 and parameters: {'max_depth': 6, 'n_estimators': 123, 'min_samples_leaf': 6}. Best is trial 8 with value: 0.6220746363061354.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:58,027]\u001b[0m Trial 16 finished with value: 0.599304237824162 and parameters: {'max_depth': 5, 'n_estimators': 65, 'min_samples_leaf': 7}. Best is trial 8 with value: 0.6220746363061354.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:58,443]\u001b[0m Trial 17 finished with value: 0.5566097406704618 and parameters: {'max_depth': 3, 'n_estimators': 150, 'min_samples_leaf': 5}. Best is trial 8 with value: 0.6220746363061354.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:58,876]\u001b[0m Trial 18 finished with value: 0.616382036685642 and parameters: {'max_depth': 6, 'n_estimators': 104, 'min_samples_leaf': 8}. Best is trial 8 with value: 0.6220746363061354.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:59,329]\u001b[0m Trial 19 finished with value: 0.5777988614800759 and parameters: {'max_depth': 4, 'n_estimators': 134, 'min_samples_leaf': 4}. Best is trial 8 with value: 0.6220746363061354.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:15:59,713]\u001b[0m Trial 20 finished with value: 0.599304237824162 and parameters: {'max_depth': 5, 'n_estimators': 108, 'min_samples_leaf': 7}. Best is trial 8 with value: 0.6220746363061354.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:00,048]\u001b[0m Trial 21 finished with value: 0.6173308032890575 and parameters: {'max_depth': 6, 'n_estimators': 85, 'min_samples_leaf': 8}. Best is trial 8 with value: 0.6220746363061354.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:00,331]\u001b[0m Trial 22 finished with value: 0.6214421252371917 and parameters: {'max_depth': 6, 'n_estimators': 70, 'min_samples_leaf': 10}. Best is trial 8 with value: 0.6220746363061354.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:00,617]\u001b[0m Trial 23 finished with value: 0.6223908918406073 and parameters: {'max_depth': 6, 'n_estimators': 68, 'min_samples_leaf': 10}. Best is trial 23 with value: 0.6223908918406073.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:00,861]\u001b[0m Trial 24 finished with value: 0.5983554712207464 and parameters: {'max_depth': 5, 'n_estimators': 61, 'min_samples_leaf': 9}. Best is trial 23 with value: 0.6223908918406073.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:01,068]\u001b[0m Trial 25 finished with value: 0.6138519924098672 and parameters: {'max_depth': 6, 'n_estimators': 46, 'min_samples_leaf': 4}. Best is trial 23 with value: 0.6223908918406073.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:01,391]\u001b[0m Trial 26 finished with value: 0.6173308032890575 and parameters: {'max_depth': 6, 'n_estimators': 78, 'min_samples_leaf': 6}. Best is trial 23 with value: 0.6223908918406073.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:01,501]\u001b[0m Trial 27 finished with value: 0.5806451612903226 and parameters: {'max_depth': 5, 'n_estimators': 26, 'min_samples_leaf': 5}. Best is trial 23 with value: 0.6223908918406073.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:01,774]\u001b[0m Trial 28 finished with value: 0.5575585072738773 and parameters: {'max_depth': 3, 'n_estimators': 99, 'min_samples_leaf': 7}. Best is trial 23 with value: 0.6223908918406073.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:02,284]\u001b[0m Trial 29 finished with value: 0.5970904490828589 and parameters: {'max_depth': 5, 'n_estimators': 134, 'min_samples_leaf': 9}. Best is trial 23 with value: 0.6223908918406073.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized on accuracy\n",
      "Optimized Random Forest:  0.6223908918406073\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    random_state=666\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "preds = rf.predict(X_test)\n",
    "\n",
    "print('Random Forest accuracy: ', accuracy_score(y_test, preds))\n",
    "# print('Random Forest f1-score: ', f1_score(y_test, preds))\n",
    "\n",
    "def create_model(trial):\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    model = RandomForestClassifier(\n",
    "        min_samples_leaf=min_samples_leaf, \n",
    "        n_estimators=n_estimators, \n",
    "        max_depth=max_depth, \n",
    "        random_state=666\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# optimizer = Optimizer('f1')\n",
    "# rf_f1_params = optimizer.optimize()\n",
    "# rf_f1_params['random_state'] = 666\n",
    "# rf_f1 = RandomForestClassifier(\n",
    "#     **rf_f1_params\n",
    "# )\n",
    "# rf_f1.fit(X_train, y_train)\n",
    "# preds = rf_f1.predict(X_test)\n",
    "\n",
    "# print('Optimized on F1 score')\n",
    "# print('Optimized Random Forest: ', accuracy_score(y_test, preds))\n",
    "# print('Optimized Random Forest f1-score: ', f1_score(y_test, preds))\n",
    "\n",
    "optimizer = Optimizer('acc')\n",
    "rf_acc_params = optimizer.optimize()\n",
    "rf_acc_params['random_state'] = 666\n",
    "rf_acc = RandomForestClassifier(\n",
    "    **rf_acc_params\n",
    ")\n",
    "rf_acc.fit(X_train, y_train);\n",
    "preds = rf_acc.predict(X_test)\n",
    "\n",
    "print('Optimized on accuracy')\n",
    "print('Optimized Random Forest: ', accuracy_score(y_test, preds))\n",
    "# print('Optimized Random Forest f1-score: ', f1_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Serialize Optimized Random Forest \n",
    "# import pickle \n",
    "# pickle.dump(rf_acc, open('optimized-rfc', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_test\n",
    "y_val = y_test\n",
    "X = X_train \n",
    "y = y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:09] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:10,314]\u001b[0m A new study created in memory with name: no-name-de0ce192-e01a-4d22-b4d3-98c9eb4b7402\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost accuracy:  0.7615433270082227\n",
      "[13:16:10] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\u001b[32m[I 2021-04-22 13:16:11,160]\u001b[0m Trial 0 finished with value: 0.7375079063883618 and parameters: {'max_depth': 5, 'n_estimators': 127, 'learning_rate': 0.6765143682861918, 'gamma': 0.7278580844622691, 'subsample': 0.9514628116505947}. Best is trial 0 with value: 0.7375079063883618.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\u001b[32m[I 2021-04-22 13:16:11,327]\u001b[0m Trial 1 finished with value: 0.5866540164452878 and parameters: {'max_depth': 2, 'n_estimators': 63, 'learning_rate': 0.048812888918720654, 'gamma': 0.0999286513283553, 'subsample': 0.5081154991364298}. Best is trial 0 with value: 0.7375079063883618.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:11] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:16:11] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:11,665]\u001b[0m Trial 2 finished with value: 0.640101201771031 and parameters: {'max_depth': 3, 'n_estimators': 112, 'learning_rate': 0.19289208375138345, 'gamma': 0.700844782081506, 'subsample': 0.29329878305864704}. Best is trial 0 with value: 0.7375079063883618.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\u001b[32m[I 2021-04-22 13:16:11,680]\u001b[0m Trial 3 finished with value: 0.5616698292220114 and parameters: {'max_depth': 5, 'n_estimators': 1, 'learning_rate': 0.1128577423459426, 'gamma': 0.11095376089207946, 'subsample': 0.24774346187216073}. Best is trial 0 with value: 0.7375079063883618.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:11] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:16:11] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:12,024]\u001b[0m Trial 4 finished with value: 0.6318785578747628 and parameters: {'max_depth': 2, 'n_estimators': 110, 'learning_rate': 0.34003500795113273, 'gamma': 0.19750323664268268, 'subsample': 0.9091886748803036}. Best is trial 0 with value: 0.7375079063883618.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:12] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:12,674]\u001b[0m Trial 5 finished with value: 0.706831119544592 and parameters: {'max_depth': 6, 'n_estimators': 80, 'learning_rate': 0.2591319234557709, 'gamma': 0.5838126604440433, 'subsample': 0.3257580838683153}. Best is trial 0 with value: 0.7375079063883618.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:12] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:13,329]\u001b[0m Trial 6 finished with value: 0.6998734977862112 and parameters: {'max_depth': 6, 'n_estimators': 94, 'learning_rate': 0.8188737086904494, 'gamma': 0.5473454656384197, 'subsample': 0.41677033473305986}. Best is trial 0 with value: 0.7375079063883618.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:13] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:13,678]\u001b[0m Trial 7 finished with value: 0.6502213788741303 and parameters: {'max_depth': 5, 'n_estimators': 56, 'learning_rate': 0.0751666364305374, 'gamma': 0.7751930009328656, 'subsample': 0.2194873035183015}. Best is trial 0 with value: 0.7375079063883618.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\u001b[32m[I 2021-04-22 13:16:13,861]\u001b[0m Trial 8 finished with value: 0.5942441492726123 and parameters: {'max_depth': 2, 'n_estimators': 74, 'learning_rate': 0.15367398663261667, 'gamma': 0.8284651504059493, 'subsample': 0.19144943108120346}. Best is trial 0 with value: 0.7375079063883618.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:13] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:16:13] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:14,158]\u001b[0m Trial 9 finished with value: 0.6448450347881088 and parameters: {'max_depth': 3, 'n_estimators': 85, 'learning_rate': 0.9023803996540212, 'gamma': 0.851788356584257, 'subsample': 0.4181401523901378}. Best is trial 0 with value: 0.7375079063883618.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:15,016]\u001b[0m Trial 10 finished with value: 0.7286527514231499 and parameters: {'max_depth': 4, 'n_estimators': 150, 'learning_rate': 0.6502310988771591, 'gamma': 0.3436433301007432, 'subsample': 0.9776081334747528}. Best is trial 0 with value: 0.7375079063883618.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:15,752]\u001b[0m Trial 11 finished with value: 0.7311827956989247 and parameters: {'max_depth': 4, 'n_estimators': 147, 'learning_rate': 0.6377276431264177, 'gamma': 0.3179553747132384, 'subsample': 0.9839509574531247}. Best is trial 0 with value: 0.7375079063883618.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:16,495]\u001b[0m Trial 12 finished with value: 0.7239089184060721 and parameters: {'max_depth': 4, 'n_estimators': 149, 'learning_rate': 0.5571306163724984, 'gamma': 0.37314707678320724, 'subsample': 0.7876624135529685}. Best is trial 0 with value: 0.7375079063883618.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:16] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:17,298]\u001b[0m Trial 13 finished with value: 0.7235926628716003 and parameters: {'max_depth': 5, 'n_estimators': 130, 'learning_rate': 0.7091039397790568, 'gamma': 0.9683903574368246, 'subsample': 0.7082435065629176}. Best is trial 0 with value: 0.7375079063883618.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:17,717]\u001b[0m Trial 14 finished with value: 0.5230866540164453 and parameters: {'max_depth': 5, 'n_estimators': 133, 'learning_rate': 0.4320740818033063, 'gamma': 0.3898418247216437, 'subsample': 0.029616583038213795}. Best is trial 0 with value: 0.7375079063883618.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\u001b[32m[I 2021-04-22 13:16:17,928]\u001b[0m Trial 15 finished with value: 0.6944971537001897 and parameters: {'max_depth': 4, 'n_estimators': 38, 'learning_rate': 0.7804340475761541, 'gamma': 0.23984808643419203, 'subsample': 0.823924978486315}. Best is trial 0 with value: 0.7375079063883618.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:18,415]\u001b[0m Trial 16 finished with value: 0.6742567994939912 and parameters: {'max_depth': 3, 'n_estimators': 132, 'learning_rate': 0.9629569132597372, 'gamma': 0.9893273690645659, 'subsample': 0.6169933589245892}. Best is trial 0 with value: 0.7375079063883618.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:19,270]\u001b[0m Trial 17 finished with value: 0.7536369386464263 and parameters: {'max_depth': 6, 'n_estimators': 110, 'learning_rate': 0.5909389632868929, 'gamma': 0.655235400813831, 'subsample': 0.9893744751073859}. Best is trial 17 with value: 0.7536369386464263.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:19] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:20,311]\u001b[0m Trial 18 finished with value: 0.7615433270082227 and parameters: {'max_depth': 6, 'n_estimators': 107, 'learning_rate': 0.4687122257988131, 'gamma': 0.6394257049197503, 'subsample': 0.8843161484878157}. Best is trial 18 with value: 0.7615433270082227.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:21,284]\u001b[0m Trial 19 finished with value: 0.7697659709044908 and parameters: {'max_depth': 6, 'n_estimators': 99, 'learning_rate': 0.47726388424495025, 'gamma': 0.6556592219046772, 'subsample': 0.8504145600313903}. Best is trial 19 with value: 0.7697659709044908.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:21,574]\u001b[0m Trial 20 finished with value: 0.7289690069576218 and parameters: {'max_depth': 6, 'n_estimators': 29, 'learning_rate': 0.47256798790445786, 'gamma': 0.4748155000815166, 'subsample': 0.640781126405347}. Best is trial 19 with value: 0.7697659709044908.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:22,542]\u001b[0m Trial 21 finished with value: 0.7545857052498419 and parameters: {'max_depth': 6, 'n_estimators': 102, 'learning_rate': 0.37218781011967905, 'gamma': 0.6138307806038967, 'subsample': 0.8534497666135381}. Best is trial 19 with value: 0.7697659709044908.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:23,331]\u001b[0m Trial 22 finished with value: 0.7590132827324478 and parameters: {'max_depth': 6, 'n_estimators': 97, 'learning_rate': 0.3729352827180554, 'gamma': 0.6222031716948176, 'subsample': 0.8611349690614417}. Best is trial 19 with value: 0.7697659709044908.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:23] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:24,073]\u001b[0m Trial 23 finished with value: 0.7507906388361796 and parameters: {'max_depth': 6, 'n_estimators': 95, 'learning_rate': 0.5199784493356745, 'gamma': 0.48023646065730063, 'subsample': 0.7379889379360859}. Best is trial 19 with value: 0.7697659709044908.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:25,013]\u001b[0m Trial 24 finished with value: 0.7583807716635041 and parameters: {'max_depth': 6, 'n_estimators': 115, 'learning_rate': 0.3121489696373104, 'gamma': 0.8993414340012825, 'subsample': 0.8773166988951857}. Best is trial 19 with value: 0.7697659709044908.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:25] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:25,640]\u001b[0m Trial 25 finished with value: 0.7400379506641366 and parameters: {'max_depth': 5, 'n_estimators': 90, 'learning_rate': 0.44432659851638934, 'gamma': 0.5378715520758524, 'subsample': 0.6237161563599487}. Best is trial 19 with value: 0.7697659709044908.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:25] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:26,122]\u001b[0m Trial 26 finished with value: 0.7454142947501581 and parameters: {'max_depth': 6, 'n_estimators': 63, 'learning_rate': 0.25355668063250864, 'gamma': 0.6648078430282481, 'subsample': 0.7605588480135438}. Best is trial 19 with value: 0.7697659709044908.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:26] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:26,824]\u001b[0m Trial 27 finished with value: 0.7463630613535737 and parameters: {'max_depth': 5, 'n_estimators': 120, 'learning_rate': 0.40097052357536467, 'gamma': 0.7764157702599557, 'subsample': 0.8992143977897817}. Best is trial 19 with value: 0.7697659709044908.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:26] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:27,643]\u001b[0m Trial 28 finished with value: 0.7482605945604048 and parameters: {'max_depth': 6, 'n_estimators': 99, 'learning_rate': 0.5138330416194699, 'gamma': 0.43763439163326456, 'subsample': 0.6918134380951609}. Best is trial 19 with value: 0.7697659709044908.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:27] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:28,197]\u001b[0m Trial 29 finished with value: 0.7160025300442757 and parameters: {'max_depth': 5, 'n_estimators': 74, 'learning_rate': 0.22406483930842497, 'gamma': 0.7520274017581641, 'subsample': 0.5482257865467624}. Best is trial 19 with value: 0.7697659709044908.\u001b[0m\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:28] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Optimized on accuracy\n",
      "Optimized XGBoost accuracy:  0.7697659709044908\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    random_state=666\n",
    ")\n",
    "xgb.fit(X, y)\n",
    "preds = xgb.predict(X_val)\n",
    "\n",
    "print('XGBoost accuracy: ', accuracy_score(y_val, preds))\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 1, 150)\n",
    "    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 1)\n",
    "    gamma = trial.suggest_uniform('gamma', 0.0000001, 1)\n",
    "    subsample = trial.suggest_uniform('subsample', 0.0001, 1.0)\n",
    "    model = XGBClassifier(\n",
    "        learning_rate=learning_rate, \n",
    "        n_estimators=n_estimators, \n",
    "        max_depth=max_depth, \n",
    "        gamma=gamma, \n",
    "        subsample=subsample,\n",
    "        random_state=666\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "optimizer = Optimizer('acc')\n",
    "xgb_acc_params = optimizer.optimize()\n",
    "xgb_acc_params['random_state'] = 666\n",
    "xgb_acc = XGBClassifier(\n",
    "    **xgb_acc_params\n",
    ")\n",
    "xgb_acc.fit(X, y)\n",
    "preds = xgb_acc.predict(X_val)\n",
    "\n",
    "print('Optimized on accuracy')\n",
    "print('Optimized XGBoost accuracy: ', accuracy_score(y_val, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Serialize Optimized Random Forest \n",
    "import pickle \n",
    "pickle.dump(xgb_acc, open('optimized-XGboost-85', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:44,906]\u001b[0m A new study created in memory with name: no-name-dd6e1be2-30b8-4d02-95a2-22cdd9e27e23\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:45,095]\u001b[0m Trial 0 finished with value: 0.7160025300442757 and parameters: {'max_depth': 5, 'n_estimators': 127, 'learning_rate': 0.6765143682861918, 'num_leaves': 2184, 'min_child_samples': 191}. Best is trial 0 with value: 0.7160025300442757.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM accuracy:  0.7754585705249842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:16:45,214]\u001b[0m Trial 1 finished with value: 0.5806451612903226 and parameters: {'max_depth': 2, 'n_estimators': 63, 'learning_rate': 0.048812888918720654, 'num_leaves': 301, 'min_child_samples': 103}. Best is trial 0 with value: 0.7160025300442757.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:45,421]\u001b[0m Trial 2 finished with value: 0.6593927893738141 and parameters: {'max_depth': 3, 'n_estimators': 112, 'learning_rate': 0.19289208375138345, 'num_leaves': 2103, 'min_child_samples': 61}. Best is trial 0 with value: 0.7160025300442757.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:45,432]\u001b[0m Trial 3 finished with value: 0.5262492093611638 and parameters: {'max_depth': 5, 'n_estimators': 1, 'learning_rate': 0.1128577423459426, 'num_leaves': 334, 'min_child_samples': 52}. Best is trial 0 with value: 0.7160025300442757.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:45,623]\u001b[0m Trial 4 finished with value: 0.6265022137887413 and parameters: {'max_depth': 2, 'n_estimators': 110, 'learning_rate': 0.34003500795113273, 'num_leaves': 594, 'min_child_samples': 183}. Best is trial 0 with value: 0.7160025300442757.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:45,789]\u001b[0m Trial 5 finished with value: 0.7432005060088551 and parameters: {'max_depth': 6, 'n_estimators': 80, 'learning_rate': 0.2591319234557709, 'num_leaves': 1752, 'min_child_samples': 67}. Best is trial 5 with value: 0.7432005060088551.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:46,048]\u001b[0m Trial 6 finished with value: 0.7463630613535737 and parameters: {'max_depth': 6, 'n_estimators': 94, 'learning_rate': 0.8188737086904494, 'num_leaves': 1643, 'min_child_samples': 85}. Best is trial 6 with value: 0.7463630613535737.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:46,151]\u001b[0m Trial 7 finished with value: 0.6688804554079696 and parameters: {'max_depth': 5, 'n_estimators': 56, 'learning_rate': 0.0751666364305374, 'num_leaves': 2326, 'min_child_samples': 46}. Best is trial 6 with value: 0.7463630613535737.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:46,226]\u001b[0m Trial 8 finished with value: 0.6132194813409234 and parameters: {'max_depth': 2, 'n_estimators': 74, 'learning_rate': 0.15367398663261667, 'num_leaves': 2486, 'min_child_samples': 40}. Best is trial 6 with value: 0.7463630613535737.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:46,316]\u001b[0m Trial 9 finished with value: 0.6850094876660342 and parameters: {'max_depth': 3, 'n_estimators': 85, 'learning_rate': 0.9023803996540212, 'num_leaves': 2556, 'min_child_samples': 85}. Best is trial 6 with value: 0.7463630613535737.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:46,499]\u001b[0m Trial 10 finished with value: 0.7394054395951929 and parameters: {'max_depth': 6, 'n_estimators': 150, 'learning_rate': 0.8981034626974946, 'num_leaves': 1240, 'min_child_samples': 134}. Best is trial 6 with value: 0.7463630613535737.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:46,616]\u001b[0m Trial 11 finished with value: 0.7488931056293485 and parameters: {'max_depth': 6, 'n_estimators': 34, 'learning_rate': 0.5386389306379887, 'num_leaves': 1598, 'min_child_samples': 7}. Best is trial 11 with value: 0.7488931056293485.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:46,736]\u001b[0m Trial 12 finished with value: 0.7488931056293485 and parameters: {'max_depth': 6, 'n_estimators': 31, 'learning_rate': 0.6180601732250435, 'num_leaves': 1237, 'min_child_samples': 3}. Best is trial 11 with value: 0.7488931056293485.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:46,809]\u001b[0m Trial 13 finished with value: 0.6878557874762808 and parameters: {'max_depth': 4, 'n_estimators': 26, 'learning_rate': 0.5312315172038903, 'num_leaves': 1011, 'min_child_samples': 3}. Best is trial 11 with value: 0.7488931056293485.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:46,969]\u001b[0m Trial 14 finished with value: 0.7511068943706515 and parameters: {'max_depth': 6, 'n_estimators': 32, 'learning_rate': 0.5261369153468621, 'num_leaves': 2975, 'min_child_samples': 4}. Best is trial 14 with value: 0.7511068943706515.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:47,044]\u001b[0m Trial 15 finished with value: 0.6761543327008223 and parameters: {'max_depth': 5, 'n_estimators': 9, 'learning_rate': 0.43126609023272977, 'num_leaves': 2877, 'min_child_samples': 18}. Best is trial 14 with value: 0.7511068943706515.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:47,141]\u001b[0m Trial 16 finished with value: 0.7043010752688172 and parameters: {'max_depth': 4, 'n_estimators': 41, 'learning_rate': 0.7214729268828322, 'num_leaves': 2981, 'min_child_samples': 25}. Best is trial 14 with value: 0.7511068943706515.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:47,218]\u001b[0m Trial 17 finished with value: 0.6521189120809614 and parameters: {'max_depth': 6, 'n_estimators': 16, 'learning_rate': 0.5861631738532908, 'num_leaves': 831, 'min_child_samples': 144}. Best is trial 14 with value: 0.7511068943706515.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:47,527]\u001b[0m Trial 18 finished with value: 0.7476280834914611 and parameters: {'max_depth': 5, 'n_estimators': 42, 'learning_rate': 0.4236875690931834, 'num_leaves': 40, 'min_child_samples': 4}. Best is trial 14 with value: 0.7511068943706515.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:47,707]\u001b[0m Trial 19 finished with value: 0.7675521821631879 and parameters: {'max_depth': 6, 'n_estimators': 57, 'learning_rate': 0.6608617260224647, 'num_leaves': 1332, 'min_child_samples': 29}. Best is trial 19 with value: 0.7675521821631879.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:47,848]\u001b[0m Trial 20 finished with value: 0.7160025300442757 and parameters: {'max_depth': 4, 'n_estimators': 54, 'learning_rate': 0.7528271964232384, 'num_leaves': 1900, 'min_child_samples': 25}. Best is trial 19 with value: 0.7675521821631879.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:48,123]\u001b[0m Trial 21 finished with value: 0.7280202403542062 and parameters: {'max_depth': 6, 'n_estimators': 22, 'learning_rate': 0.6274191372404058, 'num_leaves': 1314, 'min_child_samples': 3}. Best is trial 19 with value: 0.7675521821631879.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:48,246]\u001b[0m Trial 22 finished with value: 0.7485768500948766 and parameters: {'max_depth': 6, 'n_estimators': 47, 'learning_rate': 0.4410734887146177, 'num_leaves': 1355, 'min_child_samples': 34}. Best is trial 19 with value: 0.7675521821631879.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:48,342]\u001b[0m Trial 23 finished with value: 0.7239089184060721 and parameters: {'max_depth': 5, 'n_estimators': 33, 'learning_rate': 0.5124372773837862, 'num_leaves': 805, 'min_child_samples': 17}. Best is trial 19 with value: 0.7675521821631879.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:48,559]\u001b[0m Trial 24 finished with value: 0.756483238456673 and parameters: {'max_depth': 6, 'n_estimators': 65, 'learning_rate': 0.7973195120621179, 'num_leaves': 2756, 'min_child_samples': 14}. Best is trial 19 with value: 0.7675521821631879.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:48,776]\u001b[0m Trial 25 finished with value: 0.7390891840607211 and parameters: {'max_depth': 5, 'n_estimators': 70, 'learning_rate': 0.9954549432774109, 'num_leaves': 2769, 'min_child_samples': 36}. Best is trial 19 with value: 0.7675521821631879.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:48,969]\u001b[0m Trial 26 finished with value: 0.7425679949399114 and parameters: {'max_depth': 6, 'n_estimators': 62, 'learning_rate': 0.819356748452053, 'num_leaves': 2699, 'min_child_samples': 81}. Best is trial 19 with value: 0.7675521821631879.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:49,231]\u001b[0m Trial 27 finished with value: 0.7365591397849462 and parameters: {'max_depth': 6, 'n_estimators': 96, 'learning_rate': 0.7753743276224283, 'num_leaves': 2048, 'min_child_samples': 114}. Best is trial 19 with value: 0.7675521821631879.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:49,445]\u001b[0m Trial 28 finished with value: 0.7179000632511069 and parameters: {'max_depth': 5, 'n_estimators': 48, 'learning_rate': 0.9879898368003177, 'num_leaves': 2428, 'min_child_samples': 62}. Best is trial 19 with value: 0.7675521821631879.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:16:49,539]\u001b[0m Trial 29 finished with value: 0.6571790006325111 and parameters: {'max_depth': 3, 'n_estimators': 67, 'learning_rate': 0.6810439516395201, 'num_leaves': 1926, 'min_child_samples': 195}. Best is trial 19 with value: 0.7675521821631879.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized on accuracy\n",
      "Optimized LightGBM accuracy:  0.7675521821631879\n"
     ]
    }
   ],
   "source": [
    "lgb = LGBMClassifier(\n",
    "    random_state=666\n",
    ")\n",
    "lgb.fit(X, y)\n",
    "preds = lgb.predict(X_val)\n",
    "\n",
    "print('LightGBM accuracy: ', accuracy_score(y_val, preds))\n",
    "\n",
    "def create_model(trial):\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 1, 150)\n",
    "    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 1)\n",
    "    num_leaves = trial.suggest_int(\"num_leaves\", 2, 3000)\n",
    "    min_child_samples = trial.suggest_int('min_child_samples', 3, 200)\n",
    "    model = LGBMClassifier(\n",
    "        learning_rate=learning_rate, \n",
    "        n_estimators=n_estimators, \n",
    "        max_depth=max_depth, \n",
    "        num_leaves=num_leaves, \n",
    "        min_child_samples=min_child_samples,\n",
    "        random_state=666\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "optimizer = Optimizer('acc')\n",
    "lgb_acc_params = optimizer.optimize()\n",
    "lgb_acc_params['random_state'] = 666\n",
    "lgb_acc = LGBMClassifier(\n",
    "    **lgb_acc_params\n",
    ")\n",
    "lgb_acc.fit(X, y)\n",
    "preds = lgb_acc.predict(X_val)\n",
    "\n",
    "print('Optimized on accuracy')\n",
    "print('Optimized LightGBM accuracy: ', accuracy_score(y_val, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Serialize Optimized Random Forest \n",
    "import pickle \n",
    "pickle.dump(lgb_acc, open('optimized-LighGBM-86', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:  0.5376344086021505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(\n",
    "    random_state=666\n",
    ")\n",
    "lr.fit(X, y)\n",
    "preds = lr.predict(X_val)\n",
    "\n",
    "print('Logistic Regression: ', accuracy_score(y_val, preds))\n",
    "# print('Logistic Regression f1-score: ', f1_score(y_val, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:17:01,721]\u001b[0m A new study created in memory with name: no-name-5d163780-b145-496d-9653-d1d63d65fee2\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:01,727]\u001b[0m Trial 0 finished with value: 0.5234029095509172 and parameters: {'max_depth': 5, 'min_samples_split': 14, 'min_weight_fraction_leaf': 0.3382571679688127, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.5234029095509172.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:01,735]\u001b[0m Trial 1 finished with value: 0.5234029095509172 and parameters: {'max_depth': 6, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.20679384939326173, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.5234029095509172.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:01,743]\u001b[0m Trial 2 finished with value: 0.5455407969639469 and parameters: {'max_depth': 2, 'min_samples_split': 9, 'min_weight_fraction_leaf': 0.10012376966723874, 'min_samples_leaf': 8}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:01,752]\u001b[0m Trial 3 finished with value: 0.5234029095509172 and parameters: {'max_depth': 2, 'min_samples_split': 12, 'min_weight_fraction_leaf': 0.146614052934617, 'min_samples_leaf': 8}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:01,761]\u001b[0m Trial 4 finished with value: 0.5455407969639469 and parameters: {'max_depth': 2, 'min_samples_split': 3, 'min_weight_fraction_leaf': 0.05547683599372333, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:01,770]\u001b[0m Trial 5 finished with value: 0.5234029095509172 and parameters: {'max_depth': 2, 'min_samples_split': 12, 'min_weight_fraction_leaf': 0.17001747097731346, 'min_samples_leaf': 2}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:01,778]\u001b[0m Trial 6 finished with value: 0.5234029095509172 and parameters: {'max_depth': 6, 'min_samples_split': 16, 'min_weight_fraction_leaf': 0.26640127205304415, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:01,784]\u001b[0m Trial 7 finished with value: 0.5069576217583808 and parameters: {'max_depth': 4, 'min_samples_split': 6, 'min_weight_fraction_leaf': 0.4444496568881546, 'min_samples_leaf': 7}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:01,792]\u001b[0m Trial 8 finished with value: 0.5234029095509172 and parameters: {'max_depth': 6, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.2083560029668266, 'min_samples_leaf': 8}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:01,798]\u001b[0m Trial 9 finished with value: 0.5173940543959519 and parameters: {'max_depth': 3, 'min_samples_split': 3, 'min_weight_fraction_leaf': 0.3875964892260817, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:01,822]\u001b[0m Trial 10 finished with value: 0.5455407969639469 and parameters: {'max_depth': 3, 'min_samples_split': 6, 'min_weight_fraction_leaf': 0.00040236773156065997, 'min_samples_leaf': 10}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:01,843]\u001b[0m Trial 11 finished with value: 0.5455407969639469 and parameters: {'max_depth': 2, 'min_samples_split': 7, 'min_weight_fraction_leaf': 0.01568805253644843, 'min_samples_leaf': 5}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:01,865]\u001b[0m Trial 12 finished with value: 0.5455407969639469 and parameters: {'max_depth': 3, 'min_samples_split': 9, 'min_weight_fraction_leaf': 0.07674537248486019, 'min_samples_leaf': 5}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:01,885]\u001b[0m Trial 13 finished with value: 0.5455407969639469 and parameters: {'max_depth': 2, 'min_samples_split': 4, 'min_weight_fraction_leaf': 0.08149131277376391, 'min_samples_leaf': 10}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:01,910]\u001b[0m Trial 14 finished with value: 0.5414294750158127 and parameters: {'max_depth': 4, 'min_samples_split': 8, 'min_weight_fraction_leaf': 0.08806873318072, 'min_samples_leaf': 6}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy:  0.7017710309930424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:17:01,934]\u001b[0m Trial 15 finished with value: 0.5455407969639469 and parameters: {'max_depth': 3, 'min_samples_split': 11, 'min_weight_fraction_leaf': 0.0031660272970367304, 'min_samples_leaf': 4}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:01,954]\u001b[0m Trial 16 finished with value: 0.5455407969639469 and parameters: {'max_depth': 2, 'min_samples_split': 4, 'min_weight_fraction_leaf': 0.11411675031894318, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:01,976]\u001b[0m Trial 17 finished with value: 0.5278304870335231 and parameters: {'max_depth': 3, 'min_samples_split': 5, 'min_weight_fraction_leaf': 0.14236407805723023, 'min_samples_leaf': 9}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:02,000]\u001b[0m Trial 18 finished with value: 0.5455407969639469 and parameters: {'max_depth': 3, 'min_samples_split': 11, 'min_weight_fraction_leaf': 0.004800673280669373, 'min_samples_leaf': 6}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:02,018]\u001b[0m Trial 19 finished with value: 0.5234029095509172 and parameters: {'max_depth': 4, 'min_samples_split': 14, 'min_weight_fraction_leaf': 0.2720697793556844, 'min_samples_leaf': 6}. Best is trial 2 with value: 0.5455407969639469.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:02,044]\u001b[0m Trial 20 finished with value: 0.5493358633776091 and parameters: {'max_depth': 5, 'min_samples_split': 8, 'min_weight_fraction_leaf': 0.12060088525754976, 'min_samples_leaf': 1}. Best is trial 20 with value: 0.5493358633776091.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:02,072]\u001b[0m Trial 21 finished with value: 0.5502846299810247 and parameters: {'max_depth': 5, 'min_samples_split': 13, 'min_weight_fraction_leaf': 0.034209705073744864, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.5502846299810247.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:02,100]\u001b[0m Trial 22 finished with value: 0.5502846299810247 and parameters: {'max_depth': 5, 'min_samples_split': 14, 'min_weight_fraction_leaf': 0.03357019095932659, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.5502846299810247.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:02,129]\u001b[0m Trial 23 finished with value: 0.5506008855154966 and parameters: {'max_depth': 5, 'min_samples_split': 16, 'min_weight_fraction_leaf': 0.04069330834546158, 'min_samples_leaf': 1}. Best is trial 23 with value: 0.5506008855154966.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:02,158]\u001b[0m Trial 24 finished with value: 0.5499683744465528 and parameters: {'max_depth': 5, 'min_samples_split': 16, 'min_weight_fraction_leaf': 0.04359457877647935, 'min_samples_leaf': 2}. Best is trial 23 with value: 0.5506008855154966.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:02,186]\u001b[0m Trial 25 finished with value: 0.5506008855154966 and parameters: {'max_depth': 5, 'min_samples_split': 14, 'min_weight_fraction_leaf': 0.04095321668479658, 'min_samples_leaf': 2}. Best is trial 23 with value: 0.5506008855154966.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:02,213]\u001b[0m Trial 26 finished with value: 0.5509171410499684 and parameters: {'max_depth': 5, 'min_samples_split': 15, 'min_weight_fraction_leaf': 0.05047952168970836, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.5509171410499684.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:02,235]\u001b[0m Trial 27 finished with value: 0.5287792536369387 and parameters: {'max_depth': 5, 'min_samples_split': 16, 'min_weight_fraction_leaf': 0.18734934804813513, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.5509171410499684.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:02,252]\u001b[0m Trial 28 finished with value: 0.5075901328273245 and parameters: {'max_depth': 6, 'min_samples_split': 15, 'min_weight_fraction_leaf': 0.4982097821445296, 'min_samples_leaf': 4}. Best is trial 26 with value: 0.5509171410499684.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:02,274]\u001b[0m Trial 29 finished with value: 0.5234029095509172 and parameters: {'max_depth': 5, 'min_samples_split': 15, 'min_weight_fraction_leaf': 0.2996967847485038, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.5509171410499684.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized on accuracy\n",
      "Optimized Decision Tree accuracy:  0.5509171410499684\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(\n",
    "    random_state=666\n",
    ")\n",
    "dt.fit(X, y)\n",
    "preds = dt.predict(X_val)\n",
    "\n",
    "print('Decision Tree accuracy: ', accuracy_score(y_val, preds))\n",
    "\n",
    "def create_model(trial):\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 16)\n",
    "    min_weight_fraction_leaf = trial.suggest_uniform('min_weight_fraction_leaf', 0.0, 0.5)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    model = DecisionTreeClassifier(\n",
    "        min_samples_split=min_samples_split, \n",
    "        min_weight_fraction_leaf=min_weight_fraction_leaf, \n",
    "        max_depth=max_depth, \n",
    "        min_samples_leaf=min_samples_leaf, \n",
    "        random_state=666\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "optimizer = Optimizer('acc')\n",
    "dt_acc_params = optimizer.optimize()\n",
    "dt_acc_params['random_state'] = 666\n",
    "dt_acc = DecisionTreeClassifier(\n",
    "    **dt_acc_params\n",
    ")\n",
    "dt_acc.fit(X, y)\n",
    "preds = dt_acc.predict(X_val)\n",
    "\n",
    "print('Optimized on accuracy')\n",
    "print('Optimized Decision Tree accuracy: ', accuracy_score(y_val, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:17:09,247]\u001b[0m A new study created in memory with name: no-name-c795a788-6fd3-47a4-a912-2fb262dd46ff\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier accuracy:  0.732764073371284\n",
      "Optimized on F1-score\n",
      "Optimized Bagging Classifier accuracy:  0.732764073371284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-22 13:17:09,597]\u001b[0m Trial 0 finished with value: 0.6116382036685643 and parameters: {'n_estimators': 141, 'max_samples': 169}. Best is trial 0 with value: 0.6116382036685643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:09,938]\u001b[0m Trial 1 finished with value: 0.6087919038583175 and parameters: {'n_estimators': 136, 'max_samples': 146}. Best is trial 0 with value: 0.6116382036685643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:10,247]\u001b[0m Trial 2 finished with value: 0.5069576217583808 and parameters: {'n_estimators': 191, 'max_samples': 2}. Best is trial 0 with value: 0.6116382036685643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:10,390]\u001b[0m Trial 3 finished with value: 0.5328905755850727 and parameters: {'n_estimators': 84, 'max_samples': 9}. Best is trial 0 with value: 0.6116382036685643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:10,437]\u001b[0m Trial 4 finished with value: 0.5426944971537002 and parameters: {'n_estimators': 21, 'max_samples': 102}. Best is trial 0 with value: 0.6116382036685643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:10,543]\u001b[0m Trial 5 finished with value: 0.5907653383934219 and parameters: {'n_estimators': 41, 'max_samples': 149}. Best is trial 0 with value: 0.6116382036685643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:10,648]\u001b[0m Trial 6 finished with value: 0.5828589500316256 and parameters: {'n_estimators': 40, 'max_samples': 140}. Best is trial 0 with value: 0.6116382036685643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:10,809]\u001b[0m Trial 7 finished with value: 0.5901328273244781 and parameters: {'n_estimators': 60, 'max_samples': 155}. Best is trial 0 with value: 0.6116382036685643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:10,817]\u001b[0m Trial 8 finished with value: 0.46900695762175837 and parameters: {'n_estimators': 3, 'max_samples': 22}. Best is trial 0 with value: 0.6116382036685643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:10,867]\u001b[0m Trial 9 finished with value: 0.5234029095509172 and parameters: {'n_estimators': 24, 'max_samples': 49}. Best is trial 0 with value: 0.6116382036685643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:11,295]\u001b[0m Trial 10 finished with value: 0.622707147375079 and parameters: {'n_estimators': 144, 'max_samples': 200}. Best is trial 10 with value: 0.622707147375079.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:11,675]\u001b[0m Trial 11 finished with value: 0.627134724857685 and parameters: {'n_estimators': 147, 'max_samples': 198}. Best is trial 11 with value: 0.627134724857685.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:12,169]\u001b[0m Trial 12 finished with value: 0.6290322580645161 and parameters: {'n_estimators': 176, 'max_samples': 198}. Best is trial 12 with value: 0.6290322580645161.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:12,733]\u001b[0m Trial 13 finished with value: 0.6211258697027198 and parameters: {'n_estimators': 200, 'max_samples': 193}. Best is trial 12 with value: 0.6290322580645161.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:13,190]\u001b[0m Trial 14 finished with value: 0.6287160025300442 and parameters: {'n_estimators': 172, 'max_samples': 200}. Best is trial 12 with value: 0.6290322580645161.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:13,537]\u001b[0m Trial 15 finished with value: 0.5913978494623656 and parameters: {'n_estimators': 172, 'max_samples': 106}. Best is trial 12 with value: 0.6290322580645161.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:13,969]\u001b[0m Trial 16 finished with value: 0.6296647691334598 and parameters: {'n_estimators': 174, 'max_samples': 179}. Best is trial 16 with value: 0.6296647691334598.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:14,254]\u001b[0m Trial 17 finished with value: 0.6110056925996205 and parameters: {'n_estimators': 111, 'max_samples': 175}. Best is trial 16 with value: 0.6296647691334598.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:14,594]\u001b[0m Trial 18 finished with value: 0.5774826059456041 and parameters: {'n_estimators': 175, 'max_samples': 70}. Best is trial 16 with value: 0.6296647691334598.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:14,862]\u001b[0m Trial 19 finished with value: 0.5986717267552182 and parameters: {'n_estimators': 111, 'max_samples': 123}. Best is trial 16 with value: 0.6296647691334598.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:15,372]\u001b[0m Trial 20 finished with value: 0.624920936116382 and parameters: {'n_estimators': 200, 'max_samples': 181}. Best is trial 16 with value: 0.6296647691334598.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:15,811]\u001b[0m Trial 21 finished with value: 0.6287160025300442 and parameters: {'n_estimators': 170, 'max_samples': 200}. Best is trial 16 with value: 0.6296647691334598.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:16,261]\u001b[0m Trial 22 finished with value: 0.6201771030993042 and parameters: {'n_estimators': 161, 'max_samples': 180}. Best is trial 16 with value: 0.6296647691334598.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:16,756]\u001b[0m Trial 23 finished with value: 0.6214421252371917 and parameters: {'n_estimators': 186, 'max_samples': 164}. Best is trial 16 with value: 0.6296647691334598.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:17,057]\u001b[0m Trial 24 finished with value: 0.6065781151170145 and parameters: {'n_estimators': 125, 'max_samples': 129}. Best is trial 16 with value: 0.6296647691334598.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:17,551]\u001b[0m Trial 25 finished with value: 0.6160657811511702 and parameters: {'n_estimators': 159, 'max_samples': 188}. Best is trial 16 with value: 0.6296647691334598.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:17,950]\u001b[0m Trial 26 finished with value: 0.5809614168247944 and parameters: {'n_estimators': 186, 'max_samples': 80}. Best is trial 16 with value: 0.6296647691334598.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:18,202]\u001b[0m Trial 27 finished with value: 0.6087919038583175 and parameters: {'n_estimators': 92, 'max_samples': 162}. Best is trial 16 with value: 0.6296647691334598.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:18,675]\u001b[0m Trial 28 finished with value: 0.6239721695129665 and parameters: {'n_estimators': 159, 'max_samples': 200}. Best is trial 16 with value: 0.6296647691334598.\u001b[0m\n",
      "\u001b[32m[I 2021-04-22 13:17:19,001]\u001b[0m Trial 29 finished with value: 0.6157495256166983 and parameters: {'n_estimators': 124, 'max_samples': 171}. Best is trial 16 with value: 0.6296647691334598.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized on accuracy\n",
      "Optimized Bagging Classifier accuracy:  0.6296647691334598\n"
     ]
    }
   ],
   "source": [
    "bc = BaggingClassifier(\n",
    "    random_state=666\n",
    ")\n",
    "bc.fit(X, y)\n",
    "preds = bc.predict(X_val)\n",
    "\n",
    "print('Bagging Classifier accuracy: ', accuracy_score(y_val, preds))\n",
    "\n",
    "def create_model(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 2, 200)\n",
    "    max_samples = trial.suggest_int('max_samples', 0, 200)\n",
    "    model = BaggingClassifier(\n",
    "        n_estimators=n_estimators, \n",
    "        max_samples=max_samples, \n",
    "        random_state=666\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "print('Optimized on F1-score')\n",
    "print('Optimized Bagging Classifier accuracy: ', accuracy_score(y_val, preds))\n",
    "\n",
    "optimizer = Optimizer('acc')\n",
    "bc_acc_params = optimizer.optimize()\n",
    "bc_acc_params['random_state'] = 666\n",
    "bc_acc = BaggingClassifier(\n",
    "    **bc_acc_params\n",
    ")\n",
    "bc_acc.fit(X, y)\n",
    "preds = bc_acc.predict(X_val)\n",
    "\n",
    "print('Optimized on accuracy')\n",
    "print('Optimized Bagging Classifier accuracy: ', accuracy_score(y_val, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-419-80784060c951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-01-25 21:47:08,056]\u001b[0m A new study created in memory with name: no-name-e1efe0b4-0041-4d4c-9bc1-5a517083c958\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,059]\u001b[0m Trial 0 finished with value: 0.3888888888888889 and parameters: {'n_neighbors': 14}. Best is trial 0 with value: 0.3888888888888889.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,062]\u001b[0m Trial 1 finished with value: 0.5555555555555556 and parameters: {'n_neighbors': 4}. Best is trial 1 with value: 0.5555555555555556.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,065]\u001b[0m Trial 2 finished with value: 0.3888888888888889 and parameters: {'n_neighbors': 15}. Best is trial 1 with value: 0.5555555555555556.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,068]\u001b[0m Trial 3 finished with value: 0.6111111111111112 and parameters: {'n_neighbors': 8}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,071]\u001b[0m Trial 4 finished with value: 0.4444444444444444 and parameters: {'n_neighbors': 11}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,074]\u001b[0m Trial 5 finished with value: 0.5 and parameters: {'n_neighbors': 6}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,077]\u001b[0m Trial 6 finished with value: 0.3333333333333333 and parameters: {'n_neighbors': 21}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,080]\u001b[0m Trial 7 finished with value: 0.3888888888888889 and parameters: {'n_neighbors': 16}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,082]\u001b[0m Trial 8 finished with value: 0.3333333333333333 and parameters: {'n_neighbors': 3}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,085]\u001b[0m Trial 9 finished with value: 0.3888888888888889 and parameters: {'n_neighbors': 18}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,091]\u001b[0m Trial 10 finished with value: 0.3888888888888889 and parameters: {'n_neighbors': 9}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,097]\u001b[0m Trial 11 finished with value: 0.5555555555555556 and parameters: {'n_neighbors': 2}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,103]\u001b[0m Trial 12 finished with value: 0.5 and parameters: {'n_neighbors': 6}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,109]\u001b[0m Trial 13 finished with value: 0.5 and parameters: {'n_neighbors': 6}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,114]\u001b[0m Trial 14 finished with value: 0.4444444444444444 and parameters: {'n_neighbors': 10}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,120]\u001b[0m Trial 15 finished with value: 0.5555555555555556 and parameters: {'n_neighbors': 2}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,127]\u001b[0m Trial 16 finished with value: 0.3333333333333333 and parameters: {'n_neighbors': 25}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,133]\u001b[0m Trial 17 finished with value: 0.6111111111111112 and parameters: {'n_neighbors': 8}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,139]\u001b[0m Trial 18 finished with value: 0.4444444444444444 and parameters: {'n_neighbors': 12}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,145]\u001b[0m Trial 19 finished with value: 0.6111111111111112 and parameters: {'n_neighbors': 8}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,151]\u001b[0m Trial 20 finished with value: 0.6111111111111112 and parameters: {'n_neighbors': 8}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,157]\u001b[0m Trial 21 finished with value: 0.6111111111111112 and parameters: {'n_neighbors': 8}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,163]\u001b[0m Trial 22 finished with value: 0.5 and parameters: {'n_neighbors': 5}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,169]\u001b[0m Trial 23 finished with value: 0.4444444444444444 and parameters: {'n_neighbors': 12}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,175]\u001b[0m Trial 24 finished with value: 0.3888888888888889 and parameters: {'n_neighbors': 9}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,181]\u001b[0m Trial 25 finished with value: 0.6111111111111112 and parameters: {'n_neighbors': 8}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,188]\u001b[0m Trial 26 finished with value: 0.4444444444444444 and parameters: {'n_neighbors': 12}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,194]\u001b[0m Trial 27 finished with value: 0.3888888888888889 and parameters: {'n_neighbors': 7}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,200]\u001b[0m Trial 28 finished with value: 0.4444444444444444 and parameters: {'n_neighbors': 10}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n",
      "\u001b[32m[I 2021-01-25 21:47:08,207]\u001b[0m Trial 29 finished with value: 0.3888888888888889 and parameters: {'n_neighbors': 13}. Best is trial 3 with value: 0.6111111111111112.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy:  0.5\n",
      "Optimized KNN accuracy:  0.5\n",
      "Optimized on accuracy\n",
      "Optimized KNN accuracy:  0.6111111111111112\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X, y)\n",
    "preds = knn.predict(X_val)\n",
    "\n",
    "print('KNN accuracy: ', accuracy_score(y_val, preds))\n",
    "\n",
    "\n",
    "sampler = TPESampler(seed=0)\n",
    "def create_model(trial):\n",
    "    n_neighbors = trial.suggest_int(\"n_neighbors\", 2, 25)\n",
    "    model = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    return model\n",
    "\n",
    "\n",
    "print('Optimized KNN accuracy: ', accuracy_score(y_val, preds))\n",
    "\n",
    "\n",
    "optimizer = Optimizer('acc')\n",
    "knn_acc_params = optimizer.optimize()\n",
    "knn_acc = KNeighborsClassifier(\n",
    "    **knn_acc_params\n",
    ")\n",
    "knn_acc.fit(X, y)\n",
    "preds = knn_acc.predict(X_val)\n",
    "\n",
    "print('Optimized on accuracy')\n",
    "print('Optimized KNN accuracy: ', accuracy_score(y_val, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-21 20:37:21,918]\u001b[0m A new study created in memory with name: no-name-1b7ad30d-a1ac-4c5e-b69b-bc1dd7dcace2\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:22,091]\u001b[0m Trial 0 finished with value: 0.625 and parameters: {'n_estimators': 106, 'learning_rate': 0.8442645495290093}. Best is trial 0 with value: 0.625.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost accuracy:  0.6016949152542372\n",
      "Optimized AdaBoost accuracy:  0.6016949152542372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-21 20:37:22,259]\u001b[0m Trial 1 finished with value: 0.6292372881355932 and parameters: {'n_estimators': 102, 'learning_rate': 0.7279941282194508}. Best is trial 1 with value: 0.6292372881355932.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:22,477]\u001b[0m Trial 2 finished with value: 0.6398305084745762 and parameters: {'n_estimators': 143, 'learning_rate': 0.01319684543625025}. Best is trial 2 with value: 0.6398305084745762.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:22,578]\u001b[0m Trial 3 finished with value: 0.635593220338983 and parameters: {'n_estimators': 63, 'learning_rate': 0.049288387403100035}. Best is trial 2 with value: 0.6398305084745762.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:22,609]\u001b[0m Trial 4 finished with value: 0.625 and parameters: {'n_estimators': 16, 'learning_rate': 0.5083122726141229}. Best is trial 2 with value: 0.6398305084745762.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:22,666]\u001b[0m Trial 5 finished with value: 0.6101694915254238 and parameters: {'n_estimators': 31, 'learning_rate': 0.74428209172969}. Best is trial 2 with value: 0.6398305084745762.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:22,721]\u001b[0m Trial 6 finished with value: 0.6207627118644068 and parameters: {'n_estimators': 30, 'learning_rate': 0.7009943297898983}. Best is trial 2 with value: 0.6398305084745762.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:22,801]\u001b[0m Trial 7 finished with value: 0.6292372881355932 and parameters: {'n_estimators': 45, 'learning_rate': 0.7745922145414543}. Best is trial 2 with value: 0.6398305084745762.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:22,807]\u001b[0m Trial 8 finished with value: 0.6334745762711864 and parameters: {'n_estimators': 2, 'learning_rate': 0.11330122480489212}. Best is trial 2 with value: 0.6398305084745762.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:22,841]\u001b[0m Trial 9 finished with value: 0.6080508474576272 and parameters: {'n_estimators': 18, 'learning_rate': 0.24804439458068275}. Best is trial 2 with value: 0.6398305084745762.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:23,106]\u001b[0m Trial 10 finished with value: 0.614406779661017 and parameters: {'n_estimators': 147, 'learning_rate': 0.33547822909210473}. Best is trial 2 with value: 0.6398305084745762.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:23,231]\u001b[0m Trial 11 finished with value: 0.6398305084745762 and parameters: {'n_estimators': 67, 'learning_rate': 0.024331806530148194}. Best is trial 2 with value: 0.6398305084745762.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:23,474]\u001b[0m Trial 12 finished with value: 0.635593220338983 and parameters: {'n_estimators': 147, 'learning_rate': 0.03190792689236854}. Best is trial 2 with value: 0.6398305084745762.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:23,649]\u001b[0m Trial 13 finished with value: 0.6228813559322034 and parameters: {'n_estimators': 98, 'learning_rate': 0.19909136718321693}. Best is trial 2 with value: 0.6398305084745762.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:23,776]\u001b[0m Trial 14 finished with value: 0.635593220338983 and parameters: {'n_estimators': 70, 'learning_rate': 0.4433662333842333}. Best is trial 2 with value: 0.6398305084745762.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:23,990]\u001b[0m Trial 15 finished with value: 0.6419491525423728 and parameters: {'n_estimators': 117, 'learning_rate': 0.012194177354099445}. Best is trial 15 with value: 0.6419491525423728.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:24,199]\u001b[0m Trial 16 finished with value: 0.5783898305084746 and parameters: {'n_estimators': 126, 'learning_rate': 0.9984673274165472}. Best is trial 15 with value: 0.6419491525423728.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:24,418]\u001b[0m Trial 17 finished with value: 0.6186440677966102 and parameters: {'n_estimators': 124, 'learning_rate': 0.16876498465448772}. Best is trial 15 with value: 0.6419491525423728.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:24,634]\u001b[0m Trial 18 finished with value: 0.6207627118644068 and parameters: {'n_estimators': 131, 'learning_rate': 0.3613096285263557}. Best is trial 15 with value: 0.6419491525423728.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:24,791]\u001b[0m Trial 19 finished with value: 0.6228813559322034 and parameters: {'n_estimators': 88, 'learning_rate': 0.5877280162935219}. Best is trial 15 with value: 0.6419491525423728.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:25,056]\u001b[0m Trial 20 finished with value: 0.6313559322033898 and parameters: {'n_estimators': 150, 'learning_rate': 0.2792440822445663}. Best is trial 15 with value: 0.6419491525423728.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:25,152]\u001b[0m Trial 21 finished with value: 0.6419491525423728 and parameters: {'n_estimators': 54, 'learning_rate': 0.007674951921543155}. Best is trial 15 with value: 0.6419491525423728.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:25,241]\u001b[0m Trial 22 finished with value: 0.6207627118644068 and parameters: {'n_estimators': 52, 'learning_rate': 0.1070455352860482}. Best is trial 15 with value: 0.6419491525423728.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:25,387]\u001b[0m Trial 23 finished with value: 0.635593220338983 and parameters: {'n_estimators': 82, 'learning_rate': 0.027872210226212807}. Best is trial 15 with value: 0.6419491525423728.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:25,489]\u001b[0m Trial 24 finished with value: 0.625 and parameters: {'n_estimators': 55, 'learning_rate': 0.12263711299742862}. Best is trial 15 with value: 0.6419491525423728.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:25,677]\u001b[0m Trial 25 finished with value: 0.635593220338983 and parameters: {'n_estimators': 115, 'learning_rate': 0.2134508066594497}. Best is trial 15 with value: 0.6419491525423728.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:25,915]\u001b[0m Trial 26 finished with value: 0.6419491525423728 and parameters: {'n_estimators': 135, 'learning_rate': 0.0033746319838871656}. Best is trial 15 with value: 0.6419491525423728.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:26,097]\u001b[0m Trial 27 finished with value: 0.635593220338983 and parameters: {'n_estimators': 114, 'learning_rate': 0.10209593402544978}. Best is trial 15 with value: 0.6419491525423728.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:26,331]\u001b[0m Trial 28 finished with value: 0.6398305084745762 and parameters: {'n_estimators': 138, 'learning_rate': 0.00875349879613398}. Best is trial 15 with value: 0.6419491525423728.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:37:26,479]\u001b[0m Trial 29 finished with value: 0.5932203389830508 and parameters: {'n_estimators': 90, 'learning_rate': 0.9504709767198193}. Best is trial 15 with value: 0.6419491525423728.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized on accuracy\n",
      "Optimized AdaBoost accuracy:  0.6419491525423728\n"
     ]
    }
   ],
   "source": [
    "#Optimized KNN f1-score:  0.6306306306306305\n",
    "abc = AdaBoostClassifier(\n",
    "    random_state=66\n",
    ")\n",
    "abc.fit(X, y)\n",
    "preds = abc.predict(X_val)\n",
    "\n",
    "print('AdaBoost accuracy: ', accuracy_score(y_val, preds))\n",
    "\n",
    "def create_model(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n",
    "    learning_rate = trial.suggest_uniform('learning_rate', 0.0005, 1.0)\n",
    "    model = AdaBoostClassifier(\n",
    "        n_estimators=n_estimators, \n",
    "        learning_rate=learning_rate, \n",
    "        random_state=666\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "print('Optimized AdaBoost accuracy: ', accuracy_score(y_val, preds))\n",
    "\n",
    "optimizer = Optimizer('acc')\n",
    "abc_acc_params = optimizer.optimize()\n",
    "abc_acc_params['random_state'] = 666\n",
    "abc_acc = AdaBoostClassifier(\n",
    "    **abc_acc_params\n",
    ")\n",
    "abc_acc.fit(X, y)\n",
    "preds = abc_acc.predict(X_val)\n",
    "\n",
    "print('Optimized on accuracy')\n",
    "print('Optimized AdaBoost accuracy: ', accuracy_score(y_val, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Serialize Optimized Random Forest \n",
    "import pickle \n",
    "pickle.dump(rf_acc, open('optimized-Adaboost', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-21 20:55:09,604]\u001b[0m A new study created in memory with name: no-name-5ab0c284-e38c-4cdb-bc80-5495a846fb9a\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:09,706]\u001b[0m Trial 0 finished with value: 0.7203389830508474 and parameters: {'n_estimators': 106, 'max_depth': 6}. Best is trial 0 with value: 0.7203389830508474.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:09,800]\u001b[0m Trial 1 finished with value: 0.6779661016949152 and parameters: {'n_estimators': 102, 'max_depth': 5}. Best is trial 0 with value: 0.7203389830508474.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier accuracy:  0.8898305084745762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-21 20:55:09,925]\u001b[0m Trial 2 finished with value: 0.6016949152542372 and parameters: {'n_estimators': 143, 'max_depth': 2}. Best is trial 0 with value: 0.7203389830508474.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:09,983]\u001b[0m Trial 3 finished with value: 0.6122881355932204 and parameters: {'n_estimators': 63, 'max_depth': 2}. Best is trial 0 with value: 0.7203389830508474.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:10,001]\u001b[0m Trial 4 finished with value: 0.6588983050847458 and parameters: {'n_estimators': 16, 'max_depth': 4}. Best is trial 0 with value: 0.7203389830508474.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:10,032]\u001b[0m Trial 5 finished with value: 0.673728813559322 and parameters: {'n_estimators': 31, 'max_depth': 5}. Best is trial 0 with value: 0.7203389830508474.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:10,063]\u001b[0m Trial 6 finished with value: 0.673728813559322 and parameters: {'n_estimators': 30, 'max_depth': 5}. Best is trial 0 with value: 0.7203389830508474.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:10,107]\u001b[0m Trial 7 finished with value: 0.6800847457627118 and parameters: {'n_estimators': 45, 'max_depth': 5}. Best is trial 0 with value: 0.7203389830508474.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:10,112]\u001b[0m Trial 8 finished with value: 0.6483050847457628 and parameters: {'n_estimators': 2, 'max_depth': 2}. Best is trial 0 with value: 0.7203389830508474.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:10,131]\u001b[0m Trial 9 finished with value: 0.6334745762711864 and parameters: {'n_estimators': 18, 'max_depth': 3}. Best is trial 0 with value: 0.7203389830508474.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:10,241]\u001b[0m Trial 10 finished with value: 0.7182203389830508 and parameters: {'n_estimators': 108, 'max_depth': 6}. Best is trial 0 with value: 0.7203389830508474.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:10,352]\u001b[0m Trial 11 finished with value: 0.7203389830508474 and parameters: {'n_estimators': 111, 'max_depth': 6}. Best is trial 0 with value: 0.7203389830508474.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:10,471]\u001b[0m Trial 12 finished with value: 0.7139830508474576 and parameters: {'n_estimators': 126, 'max_depth': 6}. Best is trial 0 with value: 0.7203389830508474.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:10,563]\u001b[0m Trial 13 finished with value: 0.7288135593220338 and parameters: {'n_estimators': 88, 'max_depth': 6}. Best is trial 13 with value: 0.7288135593220338.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:10,648]\u001b[0m Trial 14 finished with value: 0.7330508474576272 and parameters: {'n_estimators': 80, 'max_depth': 6}. Best is trial 14 with value: 0.7330508474576272.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:10,726]\u001b[0m Trial 15 finished with value: 0.6567796610169492 and parameters: {'n_estimators': 78, 'max_depth': 4}. Best is trial 14 with value: 0.7330508474576272.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:10,807]\u001b[0m Trial 16 finished with value: 0.7309322033898306 and parameters: {'n_estimators': 81, 'max_depth': 6}. Best is trial 14 with value: 0.7330508474576272.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:10,871]\u001b[0m Trial 17 finished with value: 0.684322033898305 and parameters: {'n_estimators': 60, 'max_depth': 5}. Best is trial 14 with value: 0.7330508474576272.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:10,948]\u001b[0m Trial 18 finished with value: 0.635593220338983 and parameters: {'n_estimators': 83, 'max_depth': 3}. Best is trial 14 with value: 0.7330508474576272.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:11,016]\u001b[0m Trial 19 finished with value: 0.7436440677966102 and parameters: {'n_estimators': 62, 'max_depth': 6}. Best is trial 19 with value: 0.7436440677966102.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:11,077]\u001b[0m Trial 20 finished with value: 0.6483050847457628 and parameters: {'n_estimators': 58, 'max_depth': 4}. Best is trial 19 with value: 0.7436440677966102.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:11,152]\u001b[0m Trial 21 finished with value: 0.7309322033898306 and parameters: {'n_estimators': 71, 'max_depth': 6}. Best is trial 19 with value: 0.7436440677966102.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:11,204]\u001b[0m Trial 22 finished with value: 0.7351694915254238 and parameters: {'n_estimators': 49, 'max_depth': 6}. Best is trial 19 with value: 0.7436440677966102.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:11,252]\u001b[0m Trial 23 finished with value: 0.673728813559322 and parameters: {'n_estimators': 43, 'max_depth': 5}. Best is trial 19 with value: 0.7436440677966102.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:11,306]\u001b[0m Trial 24 finished with value: 0.7330508474576272 and parameters: {'n_estimators': 46, 'max_depth': 6}. Best is trial 19 with value: 0.7436440677966102.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:11,360]\u001b[0m Trial 25 finished with value: 0.6779661016949152 and parameters: {'n_estimators': 49, 'max_depth': 5}. Best is trial 19 with value: 0.7436440677966102.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:11,435]\u001b[0m Trial 26 finished with value: 0.7309322033898306 and parameters: {'n_estimators': 70, 'max_depth': 6}. Best is trial 19 with value: 0.7436440677966102.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:11,477]\u001b[0m Trial 27 finished with value: 0.7182203389830508 and parameters: {'n_estimators': 35, 'max_depth': 6}. Best is trial 19 with value: 0.7436440677966102.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:11,506]\u001b[0m Trial 28 finished with value: 0.6673728813559322 and parameters: {'n_estimators': 21, 'max_depth': 5}. Best is trial 19 with value: 0.7436440677966102.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 20:55:11,599]\u001b[0m Trial 29 finished with value: 0.7288135593220338 and parameters: {'n_estimators': 88, 'max_depth': 6}. Best is trial 19 with value: 0.7436440677966102.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized on accuracy\n",
      "Optimized ExtraTreesClassifier accuracy:  0.722457627118644\n"
     ]
    }
   ],
   "source": [
    "etcs = ExtraTreesClassifier(\n",
    "    random_state=666\n",
    ")\n",
    "etcs.fit(X, y)\n",
    "preds = etcs.predict(X_val)\n",
    "\n",
    "print('ExtraTreesClassifier accuracy: ', accuracy_score(y_val, preds))\n",
    "\n",
    "def create_model(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n",
    "    model = ExtraTreesClassifier(\n",
    "        n_estimators=n_estimators, \n",
    "        max_depth=max_depth, \n",
    "        random_state=10\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# print('Optimized ExtraTreesClassifier accuracy: ', accuracy_score(y_val, preds))\n",
    "\n",
    "optimizer = Optimizer('acc')\n",
    "et_acc_params = optimizer.optimize()\n",
    "et_acc_params['random_state'] = 0\n",
    "et_acc = ExtraTreesClassifier(\n",
    "    **et_acc_params\n",
    ")\n",
    "et_acc.fit(X, y)\n",
    "preds = et_acc.predict(X_val)\n",
    "\n",
    "print('Optimized on accuracy')\n",
    "print('Optimized ExtraTreesClassifier accuracy: ', accuracy_score(y_val, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized ExtraTreesClassifier accuracy:  0.8898305084745762\n"
     ]
    }
   ],
   "source": [
    "preds = etcs.predict(X_val)\n",
    "print('Optimized ExtraTreesClassifier accuracy: ', accuracy_score(y_val, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Serialize Optimized Extra Tree Classifier \n",
    "import pickle \n",
    "pickle.dump(et_acc, open('Optimized-Extra-Classifier-89', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in model \n",
    "import pickle\n",
    "with open('Optimized-Extra-Classifier-89', 'rb') as file:\n",
    "    oec = pickle.load(file)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 40.51 %\n"
     ]
    }
   ],
   "source": [
    "score = oec.score(X_test, y_test)\n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "y_predict = oec.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmic Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Extra Trees Classifier:': ExtraTreesClassifier(max_depth=6, n_estimators=62, random_state=0),\n",
       " 'AdaBoost Classifier:': AdaBoostClassifier(learning_rate=0.012194177354099445, n_estimators=117,\n",
       "                    random_state=666),\n",
       " 'Lgb classifier': LGBMClassifier(learning_rate=0.2747865242166644, max_depth=5,\n",
       "                min_child_samples=14, n_estimators=57, num_leaves=994,\n",
       "                random_state=666),\n",
       " 'XGB Classifier': XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1,\n",
       "               gamma=0.058153052911613476, gpu_id=-1, importance_type='gain',\n",
       "               interaction_constraints='', learning_rate=0.8324303901142126,\n",
       "               max_delta_step=0, max_depth=4, min_child_weight=1, missing=nan,\n",
       "               monotone_constraints='()', n_estimators=23, n_jobs=16,\n",
       "               num_parallel_tree=1, objective='multi:softprob', random_state=666,\n",
       "               reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "               subsample=0.7138384493818313, tree_method='exact',\n",
       "               validate_parameters=1, verbosity=None),\n",
       " 'Random Forest Classifier': RandomForestClassifier(max_depth=6, min_samples_leaf=4, n_estimators=86,\n",
       "                        random_state=666)}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put models in a dictionary\n",
    "models = {\n",
    "        \"Extra Trees Classifier:\":et_acc, \n",
    "         \"AdaBoost Classifier:\": abc_acc, \n",
    "#          \"KNN\": knn_acc, \n",
    "#          \"Logistic Regression\": lr,\n",
    "#          \"Decision Tree Classifier\": dt_acc,\n",
    "         \"Lgb classifier\": lgb_acc, \n",
    "         \"XGB Classifier\": xgb_acc, \n",
    "         \"Random Forest Classifier\": rf_acc}\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to fit and score models\n",
    "def fit_and_score(models, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Fits and evaluates given machine learning models.\n",
    "    models : a dict of different Scikit-Learn machine learning models\n",
    "    X_train : training data\n",
    "    X_test : testing data\n",
    "    y_train : labels assosciated with training data\n",
    "    y_test : labels assosciated with test data\n",
    "    \"\"\"\n",
    "    # Random seed for reproducible results\n",
    "    np.random.seed(42)\n",
    "    # Make a list to keep model scores\n",
    "    model_scores = {}\n",
    "    # Loop through models\n",
    "    for name, model in models.items():\n",
    "        # Fit the model to the data\n",
    "        model.fit(X_train, y_train)\n",
    "        # Evaluate the model and append its score to model_scores\n",
    "        model_scores[name] = model.score(X_test, y_test)\n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:44:25] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Extra Trees Classifier:': 0.722457627118644,\n",
       " 'AdaBoost Classifier:': 0.6419491525423728,\n",
       " 'Lgb classifier': 0.8686440677966102,\n",
       " 'XGB Classifier': 0.8538135593220338,\n",
       " 'Random Forest Classifier': 0.7542372881355932}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores = fit_and_score(models=models,\n",
    "                             X_train=X_train,\n",
    "                             X_test=X_test,\n",
    "                             y_train=y_train,\n",
    "                             y_test=y_test)\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_compare = pd.DataFrame(model_scores, index=['accuracy'])\n",
    "model_compare.T.plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "plt.hist(y_pred, bins=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test.hist(bins=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the model is:  0.8241525423728814\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUUAAAEGCAYAAADyuIefAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8+yak3AAAACXBIWXMAAAsTAAALEwEAmpwYAAAfIklEQVR4nO3deXxU9dn38c+VEPY17AQQlMUVFRFwqUWxFbl9HrW11vZutYhFK26tXbTto1Zv29ra2lqtikurdcXWVqzUpVgftLgAKgrUBVHZlwAJOySZ6/7jnJBJzHKSzMyZSb7v1+u8mDlz5pwrB3Lx287vZ+6OiIgE8uIOQEQkmygpiogkUVIUEUmipCgikkRJUUQkSZu4A2iuXoX5PmRQQdxhZK333+4YdwhZz9rk/K9BWu2q2MbexC5rzjlOObGTb9pcEenYhW/vedbdJzXnes2R8/8ahgwq4PVnB8UdRtY6pejIuEPIevm9escdQlZ7pfjxZp+jeHMFrz07MNKxBf0/7NXsCzZDzidFEckFToUn4g4iEiVFEUk7BxLkxoMiSooikhEJVFIUEQHAccpUfRYRCThQoeqziEgVtSmKiIQcqMiRGbmUFEUkI3KjRVFJUUQywHG1KYqIVHKHstzIiUqKIpIJRgXNenw6Y5QURSTtHEiopCgiUkUlRRGRUDB4W0lRRAQIkmKZ58ac1kqKIpJ2jlGRIxP9KymKSEYkXNVnERFAbYoiIjUYFWpTFBEJBDNvKymKiADgbuz1/LjDiERJUUQyIqE2RRGRQNDRouqziEhIHS0iIvuoo0VEpIYKDd4WEQk4RpnnRrrJjShFJKepo0VEJIljqj6LiCRTR0sLtGF1Ab+8fDAlGwvAnMlf28SZFxTv+/zPd/bm7uuLmPnOO3TrWQHAonmdufOaIsrLoVthBTc/sSyu8GP1nV+tYNzJWykpbsOFEw+MO5ysccW1Sxh7wkZKNrfl4i8du2///zlnBaedvZJEwpj/Ui/u++2IGKNsPnc0JCeZmR0I/AEYDfzI3W+u47ihwKNAT2Ah8HV335uJGKPIb+NMu2YNw0ftYuf2PC6ZNILRJ2xjvxF72LC6gDf+fxf6FFWFu700n9uuHsiND31In4FllBS33v+DnptZyKw/9OJ7v10RdyhZ5Z9PDeCpxwZx5Q2L9+0bNWYz4ydsZPqXj6G8LI9uPbLmV6DJgo6W3HjML1OpezNwGVBrMkxyE3CLuw8DtgBT0x1YY/TsW87wUbsA6Ng5waBheyheWwDAXdcVMfXHa7CkZpN//bU7x00uoc/AMgC69yrPeMzZYvFrndlWkhu/FJm0+I0ebCstqLbvv760isf/MITysuDXs3RL2zhCS7kK8iJtcctIBO6+wd3nA2V1HWNmBpwE/DncdT9wRvqja5p1K9vy4eIOHDh6J/Oe6UqvfmUccMjuasesWt6e7SX5fO+Lw5h+ygief7xHTNFKLhmw3w4OObKEWx54jZvumc/wg0vjDqnZHCPh0ba4ZVN9ridQ4u6VxalVQFGM8dRp1448brhgCBddv5r8fOfR3/XlZ498+KnjKsrhg3c6ctPMD9mzy7ji/47goNE7GXjAnhiillyRn+906VbGt88dy4hDtnL1L97m/NOOhxyZUKEu2VAKjCKbkmJkZjYNmAYwuCizP0J5GdxwwRBO+sIWjp9cykf/ac+6FW351slB58HGtQVMP2Ukt85+n979y+jaYxvtOyZo3xEOG7ed5UvbKylKvYrXt2fenD6A8f6SbnjC6NqjjK05XI0O1n3OjaSYtijNbLqZvRVuAyJ8ZRPQ3cwqs9xAYHVtB7r7DHcf4+5jevfMXDuVO/z6ysEMGr6HL164EYChB+1m5jtLeOD1pTzw+lJ69y/j9mffo7BPOcdMKmXJ/E5UlMPunca7b3Zk8HAlRKnfqy/2ZtTRmwEoGryDNgUJtm4paOBb2c6oiLg1eCazQWb2LzNbamZLzOzycH+hmT1vZh+Ef/YI95uZ3Wpmy8zsbTMbXd/501bMcvfbgdsbcbyb2b+Aswh6oM8DnkxTeE2y5PVOzPlzIUMP2sW3Th4JwJSr1zB24rZajx88fA9jJmzlookHYnnOpK9uZsiBu2s9tqW76vaPGXXMdroVlvPggiX86eZ+PPtoz7jDit33f/Y2o47aQtfuZTzwzFwevPMAnvtbEVdct4TfPz6P8rI8fn3NoeR61TlY4jRlBZhy4Ep3f8PMugALzex54BvAHHf/uZldBVwF/AA4FRgebuOAO8I/a2XunqpA62Rm/YAFQFcgAWwHDnb3rWY2G7jA3deY2f4ECbEQeBP4mrvXW7Qac3h7f/3ZQen9AXLYKUVHxh1C1svv0zvuELLaK8WPU1q2oVlZueiQ7n7xzOMjHfvjQ59e6O5jop7bzJ4Ebgu3Ce6+1sz6Ay+6+0gzuyt8/Uh4/HuVx9V2vow0yLn7OoLqcG2fTU56vRwYm4mYRCSzGjF4u5eZLUh6P8PdZ9R2oJkNAY4EXgP6JiW6dUDf8HURsDLpa5WduPElRRFp3YL5FCMXNoujlBTNrDPwF+CKsNZZdb2gOa5J1WAlRRHJgNTOvG1mBQQJ8SF3fyLcvd7M+idVnzeE+1cDyW1sdXbiQuaeaBGRViwYkpOawdvhgx73Av9x918nfTSLoIMWqnfUzgLODXuhxwOldbUngkqKIpIBKX72+Tjg68A7ZvZWuO+HwM+BmWY2FfgEODv8bDYwGVgG7ASm1HdyJUURyYhUTR3m7i9T9xilibUc78D0qOdXUhSRtAumDsuNsZZKiiKSEdkw2UMUSooiknbBLDm50a+rpCgiaRc85qekKCISUklRRKSaRjzREislRRFJO/U+i4jUoOqziEioco2WXKCkKCJp50C5SooiIlVUfRYRqZQly5dGoaQoImnXyElmY6WkKCIZoZKiiEiocpLZXKCkKCJp5xjlCXW0iIjsozZFEZFKruqziMg+alMUEalBSVFEJOQYFepoERGpoo4WEZGQq6NFRKQ6V1IUEamkCSFERKpRSTFDPljShcmHnBh3GFnL2uyIO4SsZ3m50SsamxTkMneoSCgpiojso95nEZGQo+qziEgSdbSIiFTjHncE0SgpikhGqPosIhIKep9zo5dfSVFEMkLVZxGRJLlSfc6N8qyI5DTHcI+2NcTM7jOzDWa2OGnfdWa22szeCrfJSZ9dbWbLzOw9MzulofMrKYpIRnjELYI/ApNq2X+Lux8RbrMBzOxg4BzgkPA7vzez/PpOrqQoIunn4AmLtDV4Kve5wOaIVz4deNTd97j7R8AyYGx9X1BSFJGMaET1uZeZLUjapkW8xCVm9nZYve4R7isCViYdsyrcVyclRRHJCPdoG1Ds7mOSthkRTn8HcABwBLAW+FVT46yz99nMfkc9VXx3v6ypFxWR1iXdzz67+/rK12Z2N/D38O1qYFDSoQPDfXWqb0jOgqYGKCJSjQNpTIpm1t/d14ZvzwQqe6ZnAQ+b2a+BAcBw4PX6zlVnUnT3+2tctKO772xy1CLSqqVq8LaZPQJMIGh7XAVcC0wwsyMI0u/HwIXBNX2Jmc0ElgLlwHR3r6jv/A0O3jazY4B7gc7AYDM7HLjQ3S9u4s8kIq1OtJ7lKNz9K7Xsvree428Ebox6/igdLb8BTgE2hRdYBJwQ9QIiIkBKByqmU6TH/Nx9pVm1LF9v8VNEpBrPncf8oiTFlWZ2LOBmVgBcDvwnvWGJSIuTBaXAKKJUny8CphMMeFxDMA5oehpjEpEWySJu8WqwpOjuxcB/ZyAWEWnJEnEHEE2DJUUz29/MnjKzjeHMFE+a2f6ZCE5EWojKcYpRtphFqT4/DMwE+hMMfnwceCSdQYlIy9OIx/xiFSUpdnT3P7l7ebg9CLRPd2Ai0sLk+pAcMysMX/7DzK4CHiUI+cvA7AzEJiItSRZUjaOor6NlIUESrPxJLkz6zIGr0xWUiLQ8lgWlwCjqe/Z5aCYDEZEWzA1S9JhfukV6osXMDgUOJqkt0d0fSFdQItIC5XpJsZKZXUswI8XBBG2JpwIvA0qKIhJdjiTFKL3PZwETgXXuPgU4HOiW1qhEpOXJ9d7nJLvcPWFm5WbWFdhA9ZlsW60rbniXsZ/dRMnmAi4+I1gL5/wrP2TchGLKy/JYu7IDt/x4JDu2FcQcafbIy3Nu/ftSNq0r4NrzR8QdTuwuv2YxYz+zkZLNbZn+5eMA2H/EVqb/cClt2yaoqDB+//ODeH9J93gDba40TzKbSlFKigvMrDtwN0GP9BvAK025mJlNCtdeXRYO86n5eTszeyz8/DUzG9KU62TKP//Wj/934ahq+958pQffOuNopn/haFZ/0oGzv7kipuiy0xnnr2flMg1zrfTPpwZwzaVHVds35fL3eXjGAVz61WN58M5hTLns/ZiiSy3zaFvcGkyK7n6xu5e4+53A54Dzwmp0o4Rrrd5O0CZ5MPCVcE3WZFOBLe4+DLgFuKmx18mkxQu7s620emH7zXmFJCqC2/ruoq706rsnjtCyUq9+ezn6pBKeebR33KFkjSVvFrKttHpNwh06dioHoFPncjYXt4sjtNTL9eqzmY2u7zN3f6OR1xoLLHP35eE5HiVYk3Vp0jGnA9eFr/8M3GZm5p4ND/803ue/sI65/1ACqHThtSu496eD6NhZ03HW5+6bD+T62xcy9Yr3sTznu1PGxR1SSmRDKTCK+toU61si0IGTGnmt2tZfrfm3ve8Ydy83s1KgJ1CcfFC4Duw0gPZ5nRsZRmZ8edonVJQb//p737hDyQpjTyqhZFMbli3uxKjxW+MOJ6tN/tJK7v7VSOa90I/jP7eOK65ZzI8uPjrusJovR9oU6xu8fWImA2mMcB3YGQDd2vTOuv9/Tj5jLWM/u4kfTj2cbJgfLhscMmYb408uYeyERRS0S9CxS4Lv/+ZDfnHFAXGHlnUmnraGu355IAAvP9+Xy3+8uIFv5IAsqRpHEWnwdopEWX+18phVZtaGYOjPpsyElxpHHb+Js85fyffPO4I9u/PjDidr/OEXg/jDL4K//lHjt/LFaeuUEOuweWM7DjtqC+8sLOTwozezZmWnuENKDSXFT5kPDDezoQTJ7xzgqzWOmQWcR9C7fRbwQja3J37/l0sZdXQJXbuX8cCceTx4+1DO/uYnFBQ4N96zCID3FnXltutHxhypZKvv37iIw8Zspmv3Mu6f/SIP3TWMW//nEC787rvk5Sco25vP7/6nZn9kbrIcmWTWMplzzGwyweqA+cB97n6jmV0PLHD3WWbWHvgTcCSwGTinsmOmLt3a9PZjup2Z5shzV2L7jrhDyHr5vQobPqgVm7fxMUr3bmhWO1C7QYN84OXfjnTs8u9dudDdxzTnes0R5TE/I1iOYH93v97MBgP93P31xl7M3WdTY9oxd78m6fVu4EuNPa+IZLdsGYMYRZTB278HjgEqF6DeRjDeUEQkuhxZjiBKm+I4dx9tZm8CuPsWM2ub5rhEpKXJkZJilKRYFj6N4gBm1pucWZdLRLJFrlSfoyTFW4G/An3M7EaCXuEfpzUqEWlZPHd6n6Os+/yQmS0kmD7MgDPc/T9pj0xEWpaWUlIMe5t3Ak8l73N3Tf8iItG1lKQIPE3VAlbtgaHAe8AhaYxLRFqYFtOm6O6HJb8PZ8+5OG0RiYjEqNGP+bn7G2bWMuYyEpHMaSklRTP7TtLbPGA0sCZtEYlIy9OSep+BLkmvywnaGP+SnnBEpMVqCSXFcNB2F3f/bobiEZEWyEhdR4uZ3QecBmxw90PDfYXAY8AQ4GPg7PDpOwN+C0wmGEXzjYZWDajz2Wcza+PuFcBxKfg5RKS1S90aLX8EJtXYdxUwx92HA3PC9xCsCTU83KYBdzR08vpKiq8TtB++ZWazgMeBffNQufsTkcIXEUnhLDnuPreWlT5PByaEr+8HXgR+EO5/IJyX9VUz625m/d19bV3nj9Km2J5g9uuTqBqv6ICSoohEF72jpZeZLUh6PyNcgqQ+fZMS3TqgcnGk2taGKgKalBT7hD3Pi6lKhpVypMlURLJFI0qKxc2ZZNbd3azp5dL6kmI+0JnaV15SUhSRxklv1lhfWS02s/7AhnB/lLWhqqkvKa519+ubF6eICJlYza9yfaefh38+mbT/knCd+XFAaX3tiVB/Uox/ClwRaTFSOCTnEYJOlV5mtgq4liAZzjSzqcAnwNnh4bMJhuMsIxiSM6Wh89eXFCc2PWwRkRpS1/v8lTo++lTOCnudpzfm/HUmRXff3JgTiYjUpyU95ici0jzpb1NMGSVFEUk7I3c6KZQURSQzVFIUEanSYmbeFhFJCSVFEZFQC5tkVkSk+VRSFBGpojZFEZFkSoqZ4e0KSAwZEHcYWcvfXBJ3CFlv6Y0D4w4hq+3+SduUnEclRRGRSk5jJpmNlZKiiKRdKheuSjclRRHJDCVFEZEq5rmRFZUURST9NEuOiEh1alMUEUmix/xERJKppCgiEnJVn0VEqlNSFBEJaPC2iEgNlsiNrKikKCLpp3GKIiLVaUiOiEgylRRFRKqoo0VEpJIDmhBCRKSK2hRFREIapygiksxd1WcRkWQqKYqIJFNSFBGpopKiiEglBypyIysqKYpIRqSypGhmHwPbgAqg3N3HmFkh8BgwBPgYONvdtzT23HmpC1NEpB6VPdANbdGd6O5HuPuY8P1VwBx3Hw7MCd83mpKiiGSEebStGU4H7g9f3w+c0ZSTKCmKSPp5I7boZ3zOzBaa2bRwX193Xxu+Xgf0bUqoalMUkbQzwKJ3tPQyswVJ72e4+4waxxzv7qvNrA/wvJm9m/yhu7tZ08qdSooikhEWvb2wOKmdsFbuvjr8c4OZ/RUYC6w3s/7uvtbM+gMbmhKnqs8ikn4prD6bWScz61L5Gvg8sBiYBZwXHnYe8GRTQlVJsYkKCiq4+abnKShIkJ/vvPTvQTz40CjAOe/ct/nM8StIJIynnx7Ok0+NjDvcrDBmwlYuumEN+XnOPx4pZOZtTWryyXl97/uITotKqejahk9uOBSAnk+spvNbJbhBRdcC1p0/hIoebcnbUU6/+z6mYOMevMBYN2Uoewd2iPknaIqUPvvcF/irmUGQwx5292fMbD4w08ymAp8AZzfl5BlLimZ2H3AasMHdD63lcwN+C0wGdgLfcPc3MhVfY5WV5fGDH05k9+4C8vMT/OqXz7NgwQAGDSqld68dfPPC03A3unXbHXeoWSEvz5n+09Vcfc7+FK8t4HezP+DVZ7ux4oP2cYeWcVuP60XJxD70u+ejffu2nNqPTV8oAqD78+vp+dRaNpy7H4VPr2X34I6suXQYBWt30ffBFaz6Xm7+J5uqcYruvhw4vJb9m4CJzT1/JqvPfwQm1fP5qcDwcJsG3JGBmJrB2L27AIA2bRK0yU/gwGmTP+ChRw7D3QAoLW19v/S1GXnkTtZ83JZ1K9pRXpbHi09255hTSuMOKxa7RnaholP18kiiQ/6+17a3auLBtmt2s+ugLgCU9e9Am+K95JeWZSbQVEv9OMW0yFhJ0d3nmtmQeg45HXjA3R141cy6VzaaZibCxsvLS/C73z7DgP7beerp4bz3Xi/699/OZ0/4hGOPWUVpaTvuuOso1qzpGneosevZr4yNa9rue1+8toADR++MMaLs0/Mvq+g6bxOJjvn7SoN7BnWg88It7BrRhfbLt1OwaQ9ttuyloltBzNE2kjeq9zlW2dTRUgSsTHq/KtyXtRKJPKZfOpmvnXcGI0dsYr/9SigoSLB3bz6XXTGJZ54dxncufy3uMCVHbPriQD761eFsHd+T7i8EHadbJvcnb2cFg69dQvc5G9gzuCPkWcyRNlFqxymmTTYlxcjMbJqZLTCzBWXl8Zc2duxoy6K3+zLmqLUUF3fk3/MGAfDveQMZOrQk3uCyxKZ1BfQesHff+179yyhem2OlnQzZNr6QzguDR3YTHfJZP3UoK35yCOsuGEr+tnLKereLOcKmMfdIW9yyKSmuBgYlvR8Y7vsUd5/h7mPcfUxBm44ZCa6mbl1306lT8Evetm05o49Yx8qVXZn36kAOH7UegFGHbWD16i6xxJdt3nurI0VD99J30B7aFCSYcHoJrz7XLe6wskbB+qoOuc5vlrC3X9DDnLezHMqDNsZuc4vZNaJLtfbHnKI2xUabBVxiZo8C44DSbG5PLCzcxZXfeZX8PMfMmfvyYF6fX8SSpb35wffmceYZ77J7VxtuuXVc3KFmhUSFcfuPivjpw8vJy4fnHi3kk/dbZydUvzuX0/G9beRvL2folYvYdPoAOr1TStt1u8GMsp5t2XDufkDQ0dLv3qCXek9RB9ZPGRJj5M3ggBauqs7MHgEmEDzCswq4FigAcPc7gdkEw3GWEQzJmZKp2Jrio497cMllp35q/44dbbnmugmZDygHzH+hK/NfUKfTuov2/9S+rSf0rvXY3cM68/HPDkt3SGlnZEfVOIpM9j5/pYHPHZieoXBEJNMSuVFUzKbqs4i0VKo+i4hUp+qziEgyJUURkUrZMdwmCiVFEUk/reYnIlKd2hRFRJIpKYqIhBxIKCmKiITU0SIiUp2SoohIyIGK3HikRUlRRDLAwZUURUSqqPosIhJS77OISA0qKYqIJFFSFBEJuUNFRdxRRKKkKCKZoZKiiEgSJUURkUqu3mcRkX0cXIO3RUSS6DE/EZGQu5Y4FRGpRh0tIiJVXCVFEZFKmmRWRKSKJoQQEanigOfIY355cQcgIq2Ah5PMRtkaYGaTzOw9M1tmZlelOlSVFEUkIzwF1WczywduBz4HrALmm9ksd1/a7JOHVFIUkcxITUlxLLDM3Ze7+17gUeD0VIZpniM9QnUxs43AJ3HHkaQXUBx3EFlM96dh2XaP9nP33s05gZk9Q/BzRdEe2J30foa7zwjPcxYwyd0vCN9/HRjn7pc0J75kOV99bu5fVqqZ2QJ3HxN3HNlK96dhLfEeufukuGOIStVnEcklq4FBSe8HhvtSRklRRHLJfGC4mQ01s7bAOcCsVF4g56vPWWhG3AFkOd2fhuke1cHdy83sEuBZIB+4z92XpPIaOd/RIiKSSqo+i4gkUVIUEUmipNhEZnagmb1iZnvM7Lv1HDfUzF4LH0l6LGwcbvEaehTLzNqF92NZeH+GxBBmbMzsPjPbYGaL6/jczOzW8P68bWajMx1ja6Wk2HSbgcuAmxs47ibgFncfBmwBpqY7sLglPYp1KnAw8BUzO7jGYVOBLeF9uYXgPrUmfwTqG7t3KjA83KYBd2QgJkFJscncfYO7zwfK6jrGzAw4CfhzuOt+4Iz0Rxe7KI9inU5wPyC4PxPD+9UquPtcgv9Y63I68IAHXgW6m1n/zETXuikppldPoMTdy8P3q4CiGOPJlCJgZdL72n7ufceE96eU4H5JIMo9lDRQUhQRSaKk2AhmNt3M3gq3ARG+somg2lM5SD7ljyRlqSiPYu07Jrw/3QjulwTS/jib1E5JsRHc/XZ3PyLc1kQ43oF/AWeFu84DnkxnjFkiyqNYswjuBwT35wXXkwTJZgHnhr3Q44FSd18bd1CtgZ5oaSIz6wcsALoCCWA7cLC7bzWz2cAF7r7GzPYn6GgoBN4Evubue+KKO1PMbDLwG6oexbrRzK4HFrj7LDNrD/wJOJKgw+Ecd18eW8AZZmaPABMIptNaD1wLFAC4+51hp9NtBD3UO4Ep7r4gnmhbFyVFEZEkqj6LiCRRUhQRSaKkKCKSRElRRCSJkqKISBIlxRbOzCrCweaLzexxM+vYjHP9MVxNDTO7p5ZJHpKPnWBmxzbhGh+b2adWfatrf41jtjfyWtfVN8ORtE5Kii3frnCw+aHAXuCi5A+TnrZpFHe/oIEFyCcAjU6KInFTUmxdXgKGhaW4l8xsFrDUzPLN7JdmNj+cu+9C2Den323hvIj/BPpUnsjMXjSzMeHrSWb2hpktMrM54dyIFwHfDkupnzGz3mb2l/Aa883suPC7Pc3sOTNbYmb3AA3OlGNmfzOzheF3ptX47JZw/xwz6x3uO8DMngm/85KZHZiSuyktkhauaiXCEuGpwDPhrtHAoe7+UZhYSt39aDNrB/zbzJ4jeNpkJMGciH2BpcB9Nc7bG7gbOCE8V6G7bzazO4Ht7n5zeNzDBPNKvmxmgwkWHjqI4EmOl939ejP7L6LNN3l+eI0OwHwz+4u7bwI6ETwx820zuyY89yUEC0Fd5O4fmNk44PcEU7qJfIqSYsvXwczeCl+/BNxLUK193d0/Cvd/HhhV2V5IMDnDcOAE4BF3rwDWmNkLtZx/PDC38lzuXtccgScDBydNmdjVzDqH1/hC+N2nzWxLhJ/pMjM7M3w9KIx1E8Hjlo+F+x8EngivcSzweNK120W4hrRSSoot3y53PyJ5R5gcdiTvAi5192drHDc5hXHkAePdfXctsURmZhMIEuwx7r7TzF4E2tdxuIfXLal5D0TqojZFgaAq+y0zKwAwsxFm1gmYC3w5bHPsD5xYy3dfBU4ws6HhdwvD/duALknHPQdcWvnGzI4IX84FvhruOxXo0UCs3QiWMdgZtg2OT/osj6oZib5KUC3fCnxkZl8Kr2FmdngD15BWTElRAO4haC98w4KFlO4iqEX8Ffgg/OwB4JWaX3T3jQRriDxhZouoqr4+BZxZ2dFCsJ7NmLAjZylVveA/IUiqSwiq0SsaiPUZoI2Z/Qf4OUFSrrQDGBv+DCcB14f7/xuYGsa3hE8vjSCyj2bJERFJopKiiEgSJUURkSRKiiIiSZQURUSSKCmKiCRRUhQRSaKkKCKS5H8BwvS3qhUylqMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Accuracy score of the model is: \", accuracy_score(y_test, y_pred))\n",
    "plot_confusion_matrix(rfc, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Algorithm was the Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super Learner Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/andrewilliams/Documents/Dev/DS-2.9-Technical-Seminar/my_env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SuperLearner accuracy:  0.8516949152542372\n"
     ]
    }
   ],
   "source": [
    "model = SuperLearner(\n",
    "    folds=5, \n",
    "    random_state=666\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    [\n",
    "        bc, \n",
    "        lgb, \n",
    "        xgb, \n",
    "        rf,  \n",
    "    ]\n",
    ")\n",
    "\n",
    "model.add_meta(\n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "preds = model.predict(X_val)\n",
    "\n",
    "print('SuperLearner accuracy: ', accuracy_score(y_val, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dt_acc_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-264-106b61651598>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m'OAXGB'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mxgb_acc_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m'OALGBM'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlgb_acc_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;34m'OADT'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdt_acc_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;34m'OAKNN'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mknn_acc_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m'OABC'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbc_acc_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dt_acc_params' is not defined"
     ]
    }
   ],
   "source": [
    "mdict = {\n",
    "    'RF': RandomForestClassifier(random_state=666),\n",
    "    'XGB': XGBClassifier(random_state=666),\n",
    "    'LGBM': LGBMClassifier(random_state=666),\n",
    "    'DT': DecisionTreeClassifier(random_state=666),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'BC': BaggingClassifier(random_state=666),\n",
    "    'OARF': RandomForestClassifier(**rf_acc_params),\n",
    "    'OAXGB': XGBClassifier(**xgb_acc_params),\n",
    "    'OALGBM': LGBMClassifier(**lgb_acc_params),\n",
    "    'OADT': DecisionTreeClassifier(**dt_acc_params),\n",
    "    'OAKNN': KNeighborsClassifier(**knn_acc_params),\n",
    "    'OABC': BaggingClassifier(**bc_acc_params),\n",
    "    'OAABC': AdaBoostClassifier(**abc_acc_params),\n",
    "    'OAET': ExtraTreesClassifier(**et_acc_params),\n",
    "    'LR': LogisticRegression(random_state=666),\n",
    "    'ABC': AdaBoostClassifier(random_state=666),\n",
    "    'SGD': SGDClassifier(random_state=666), \n",
    "    'ET': ExtraTreesClassifier(random_state=666),\n",
    "    'MLP': MLPClassifier(random_state=666),\n",
    "    'GB': GradientBoostingClassifier(random_state=666),\n",
    "    'RDG': RidgeClassifier(random_state=666),\n",
    "    'PCP': Perceptron(random_state=666),\n",
    "    'PAC': PassiveAggressiveClassifier(random_state=666)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# yt = le.fit_transform(y_train)\n",
    "# yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(trial):\n",
    "#     model_names = list()\n",
    "#     models_list = [\n",
    "#         'RF', 'XGB', 'LGBM', 'DT', \n",
    "#         'KNN', 'BC', 'OARF', #'OFRF', \n",
    "#         'OAXGB', 'OFXGB', 'OALGBM', \n",
    "#         'OFLGBM', 'OADT', 'OFDT', \n",
    "#         'OAKNN', \n",
    "#         'OFBC', 'OAABC', 'OFABC', \n",
    "#         'OAET', 'OFET', 'LR', \n",
    "#         'ABC', 'SGD', 'ET', \n",
    "#         'MLP', 'GB', 'RDG', \n",
    "#         'PCP', 'PAC'\n",
    "#     ]\n",
    "    \n",
    "#     head_list = [\n",
    "#         'RF', \n",
    "#         'XGB', \n",
    "#         'LGBM', \n",
    "#         'DT', \n",
    "#         'KNN', \n",
    "# #       'BC', \n",
    "#         'LR', \n",
    "#         'ABC', \n",
    "#         'SGD', \n",
    "#         'ET', \n",
    "#         'MLP', \n",
    "#         'GB', \n",
    "#         'RDG', \n",
    "#         'PCP', \n",
    "#         'PAC'\n",
    "#     ]\n",
    "    \n",
    "#     n_models = trial.suggest_int(\"n_models\", 2, 6)\n",
    "#     for i in range(n_models):\n",
    "#         model_item = trial.suggest_categorical('model_{}'.format(i), models_list)\n",
    "#         if model_item not in model_names:\n",
    "#             model_names.append(model_item)\n",
    "    \n",
    "#     folds = trial.suggest_int(\"folds\", 2, 6)\n",
    "    \n",
    "#     model = SuperLearner(\n",
    "#         folds=folds, \n",
    "#         random_state=666\n",
    "#     )\n",
    "    \n",
    "#     models = [\n",
    "#         mdict[item] for item in model_names\n",
    "#     ]\n",
    "#     model.add(models)\n",
    "#     head = trial.suggest_categorical('head', head_list)\n",
    "#     model.add_meta(\n",
    "#         mdict[head]\n",
    "#     )\n",
    "        \n",
    "#     return model\n",
    "\n",
    "# def objective(trial):\n",
    "#     model = create_model(trial)\n",
    "#     model.fit(X, y_train)\n",
    "#     preds = model.predict(X_val)\n",
    "#     score = accuracy_score(y_val, preds)\n",
    "#     return score\n",
    "\n",
    "# study = optuna.create_study(\n",
    "#     direction=\"maximize\", \n",
    "#     sampler=sampler\n",
    "# )\n",
    "\n",
    "# study.optimize(\n",
    "#     objective, \n",
    "#     n_trials=50\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = study.best_params\n",
    "\n",
    "# head = params['head']\n",
    "# folds = params['folds']\n",
    "# del params['head'], params['n_models'], params['folds']\n",
    "# result = list()\n",
    "# for key, value in params.items():\n",
    "#     if value not in result:\n",
    "#         result.append(value)\n",
    "        \n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SuperLearner(\n",
    "#     folds=folds, \n",
    "#     random_state=666\n",
    "# )\n",
    "\n",
    "# models = [\n",
    "#     mdict[item] for item in result\n",
    "# ]\n",
    "# model.add(models)\n",
    "# model.add_meta(mdict[head])\n",
    "\n",
    "# model.fit(X, y)\n",
    "\n",
    "# preds = model.predict(X_val)\n",
    "\n",
    "# print('Optimized SuperLearner accuracy: ', accuracy_score(y_val, preds))\n",
    "# print('Optimized SuperLearner f1-score: ', f1_score(y_val, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
